import json
import asyncio
from typing import List, Dict, Any
import aiohttp

from langchain_core.messages import HumanMessage, AIMessage
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.output_parsers import JsonOutputParser



from langflow.base.models.model_input_constants import (
    ALL_PROVIDER_FIELDS,

    MODEL_PROVIDERS,
    MODEL_PROVIDERS_DICT,
    MODELS_METADATA,
)
from langflow.components.helpers.memory import MemoryComponent
from langflow.components.langchain_utilities.tool_calling import ToolCallingAgentComponent

from langflow.custom.utils import update_component_build_config

from langflow.io import BoolInput, DropdownInput, HandleInput, IntInput, MultilineInput, Output, StrInput
from langflow.logging import logger
from langflow.schema.dotdict import dotdict
from langflow.utils.constants import MESSAGE_SENDER_AI, MESSAGE_SENDER_NAME_AI
from langflow.schema.message import Message


def create_prompt_from_component(prompt_component, default_system_message: str = "") -> ChatPromptTemplate:
    """
    í”„ë¡¬í”„íŠ¸ ì»´í¬ë„ŒíŠ¸ì—ì„œ ChatPromptTemplateì„ ìƒì„±í•˜ëŠ” í—¬í¼ í•¨ìˆ˜
    
    Args:
        prompt_component: Langflow Prompt Componentì—ì„œ ì¶œë ¥ëœ Message ê°ì²´
        default_system_message: ê¸°ë³¸ ì‹œìŠ¤í…œ ë©”ì‹œì§€
    
    Returns:
        ChatPromptTemplate: LangChain í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
    """
    if not prompt_component:
        logger.warning("prompt_component is None or empty")
        return None
        
    try:
        # Message ê°ì²´ì—ì„œ template ì¶”ì¶œ
        if hasattr(prompt_component, 'template'):
            template_content = prompt_component.template
        elif hasattr(prompt_component, 'text'):
            template_content = prompt_component.text
        else:
            logger.warning("Prompt component format not recognized")
            return None
        
        # ChatPromptTemplate ìƒì„± (ë³€ìˆ˜ëŠ” ëŸ°íƒ€ì„ì— ì£¼ì…)
        # chat_history ë³€ìˆ˜ê°€ í¬í•¨ëœ ê²½ìš° MessagesPlaceholder ì¶”ê°€
        if "{chat_history}" in template_content:
            if default_system_message:
                return ChatPromptTemplate.from_messages([
                    ("system", default_system_message),
                    MessagesPlaceholder(variable_name="chat_history"),
                    ("human", template_content)
                ])
            else:
                return ChatPromptTemplate.from_messages([
                    MessagesPlaceholder(variable_name="chat_history"),
                    ("human", template_content)
                ])
        else:
            if default_system_message:
                return ChatPromptTemplate.from_messages([
                    ("system", default_system_message),
                    ("human", template_content)
                ])
            else:
                return ChatPromptTemplate.from_messages([
                    ("human", template_content)
                ])
            
    except Exception as e:
        logger.error(f"Error creating prompt from component: {e}")
        return None



MODEL_PROVIDERS_LIST = ["Anthropic", "Google Generative AI", "Groq", "OpenAI"]


class A2APlannerComponent(ToolCallingAgentComponent):
    display_name: str = "A2A Planner"
    description: str = "A2A Discovery ì„œë¹„ìŠ¤ì™€ ì—°ë™í•˜ì—¬ ì—ì´ì „íŠ¸ ëª©ë¡ì„ ì¡°íšŒí•˜ê³  ì‘ì—… ê³„íšì„ ìˆ˜ë¦½í•˜ëŠ” í”Œë˜ë„ˆ"
    documentation: str = "https://docs.langflow.org/agents"
    icon = "bot"
    beta = False
    name = "A2A Planner"



    inputs = [
        # LLM ì—°ê²°ì„ ìœ„í•œ ì…ë ¥ í•„ë“œ
        DropdownInput(
            name="agent_llm",
            display_name="Model Provider",
            info="The provider of the language model that the agent will use to generate responses.",
            options=[*MODEL_PROVIDERS_LIST, "Custom"],
            value="OpenAI",
            real_time_refresh=True,
            input_types=["LanguageModel"],
            options_metadata=[MODELS_METADATA[key] for key in MODEL_PROVIDERS_LIST] + [{"icon": "brain"}],
        ),
        # OpenAI ëª¨ë¸ ì„¤ì • ì…ë ¥ë“¤
        *MODEL_PROVIDERS_DICT["OpenAI"]["inputs"],

        StrInput(
            name="a2a_api_base",
            display_name="A2A API Base URL",
            info="A2A Discovery ì„œë¹„ìŠ¤ì˜ API ê¸°ë³¸ URL (ì˜ˆ: http://localhost:8000, https://api.example.com)",
            value="http://localhost:8000",
            advanced=False,
        ),
        MultilineInput(
            name="system_prompt",
            display_name="Agent Instructions",
            info="System Prompt: A2A ì—ì´ì „íŠ¸ì˜ ì´ˆê¸° ì§€ì‹œì‚¬í•­ê³¼ ì»¨í…ìŠ¤íŠ¸",
            value="""ë‹¹ì‹ ì€ A2A Discovery ì„œë¹„ìŠ¤ì™€ ì—°ë™í•˜ì—¬ ë©€í‹° ì—ì´ì „íŠ¸ ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ” A2A ì—ì´ì „íŠ¸ì…ë‹ˆë‹¤.

ì£¼ìš” ì—­í• :
1. A2A Discovery ì„œë¹„ìŠ¤ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ì—ì´ì „íŠ¸ ëª©ë¡ì„ ì¡°íšŒ
2. ì‚¬ìš©ì ì‘ì—…ì„ ë¶„ì„í•˜ê³  ì‘ì—… ë¶„í•  ê³„íš ìˆ˜ë¦½
3. ì ì ˆí•œ ì—ì´ì „íŠ¸ì—ê²Œ ì‘ì—… ìœ„ì„ ë° ì‹¤í–‰
4. ê²°ê³¼ ë¶„ì„ ë° ê°œì„ ì•ˆ ì œì‹œ (Planning-Dispatcher-Critic íŒ¨í„´)

ì‘ì—… ê³„íš ìˆ˜ë¦½ ì‹œ ë‹¤ìŒ ì›ì¹™ì„ ì¤€ìˆ˜í•˜ì„¸ìš”:
- ëª©í‘œë¥¼ ëª…í™•í•œ ì‚°ì¶œë¬¼ ì¤‘ì‹¬ìœ¼ë¡œ ë¶„í•´ (30-90ë¶„ ë‚´ ì™„ë£Œ ê°€ëŠ¥í•œ í¬ê¸°)
- ê° íƒœìŠ¤í¬ì— ë‹´ë‹¹ ì—ì´ì „íŠ¸/í•„ìš” ìŠ¤í‚¬/ì…ì¶œë ¥/ì™„ë£Œ ê¸°ì¤€/ì˜ì¡´ì„±/ì¶”ì • ì‹œê°„ ëª…ì‹œ
- ìœ„ìƒì •ë ¬ ê°€ëŠ¥í•œ ìˆœì„œì˜ ì‹¤í–‰ ê³„íš ì œì‹œ
- ë¦¬ìŠ¤í¬ì™€ ëŒ€ì•ˆ í¬í•¨, ë³‘ë ¬í™” ê°€ëŠ¥ êµ¬ê°„ ëª…ì‹œ
- OUTPUT_SCHEMA í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µ""",
            advanced=False,
        ),
        IntInput(
            name="n_messages",
            display_name="Number of Chat History Messages",
            value=100,
            info="Number of chat history messages to retrieve.",
            advanced=True,
            show=True,
        ),
        # ChatInput ì—°ê²°ì„ ìœ„í•œ ì…ë ¥ í•„ë“œ
        StrInput(
            name="input_value",
            display_name="Input Value",
            info="ì‚¬ìš©ì ì…ë ¥ ë©”ì‹œì§€ ë˜ëŠ” ChatInputì—ì„œ ì—°ê²°ëœ ê°’",
            value="",
            advanced=False,
            input_types=["Message"],
        ),
        # Agent Description ì…ë ¥ í•„ë“œ
        StrInput(
            name="agent_description",
            display_name="Agent Description",
            info="ì—ì´ì „íŠ¸ì— ëŒ€í•œ ì„¤ëª…",
            value="A helpful assistant with access to the following tools:",
            advanced=True,
        ),
        # ê¸°ë³¸ ì—ì´ì „íŠ¸ ì…ë ¥ë“¤ (max_iterations ì œì™¸)
        BoolInput(
            name="handle_parsing_errors",
            display_name="Handle Parsing Errors",
            value=True,
            info="Whether to handle parsing errors.",
            advanced=True,
        ),
        BoolInput(
            name="verbose",
            display_name="Verbose",
            value=False,
            info="Whether to run in verbose mode.",
            advanced=True,
        ),
        # ë¶€ëª¨ í´ë˜ìŠ¤ì˜ max_iterationsë¥¼ ë¬´ì‹œí•˜ê³  A2A ì „ìš© ì‚¬ìš©
        IntInput(
            name="a2a_max_iterations",
            display_name="A2A Maximum Iterations",
            value=3,
            info="Planning-Dispatcher-Critic ì‚¬ì´í´ì˜ ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜ (ê¸°ë³¸ê°’: 3íšŒ)",
            advanced=True,
        ),
        BoolInput(
            name="add_current_date_tool",
            display_name="Current Date",
            advanced=True,
            info="If true, will add a tool to the agent that returns the current date.",
            value=True,
        ),

        # Prompt Component ì…ë ¥ í•„ë“œë“¤ (í•„ìˆ˜)
        HandleInput(
            name="work_plan_prompt",
            display_name="Work Plan Prompt (í•„ìˆ˜)",
            info="ì‘ì—… ê³„íš ìˆ˜ë¦½ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ (Prompt Template ì»´í¬ë„ŒíŠ¸ ì—°ê²° í•„ìˆ˜)",
            advanced=False,
            input_types=["Message"],
            required=True,
        ),



    ]
    outputs = [
        Output(name="response", display_name="Work Plan", method="build_agent")
    ]

    


    async def _initialize_llm(self):
        """LLM ë° í•„ìˆ˜ ì†ì„± ì´ˆê¸°í™”"""
        try:
            # chat_history ì´ˆê¸°í™”
            if not hasattr(self, 'chat_history'):
                self.chat_history = []
                logger.info("Initialized empty chat_history")
            
            if hasattr(self, 'llm') and self.llm is not None:
                logger.info("LLM already initialized")
                return
                
            # agent_llmì´ ì„¤ì •ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
            if hasattr(self, 'agent_llm') and self.agent_llm is not None:
                logger.info(f"Initializing LLM with agent_llm: {type(self.agent_llm)}")
                self.llm = self.agent_llm
            else:
                logger.warning("No agent_llm provided, LLM will not be available")
                self.llm = None
                
        except Exception as e:
            logger.error(f"Failed to initialize LLM: {e}")
            self.llm = None

    async def run_a2a_agent(self) -> Message:
        """A2A ì—ì´ì „íŠ¸ì˜ ë©”ì¸ ì‹¤í–‰ ë¡œì§"""
        try:
            # 1. A2A Discovery ì„œë¹„ìŠ¤ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ì—ì´ì „íŠ¸ ëª©ë¡ ì¡°íšŒ
            available_agents = await self.get_available_agents()

            # 2. ì‘ì—… ë¶„ì„ ë° ê³„íš ìˆ˜ë¦½
            work_plan = await self.create_work_plan(available_agents)

            # 3. Planning-Dispatcher-Critic ì‚¬ì´í´ ì‹¤í–‰
            final_result = await self.execute_planning_dispatcher_critic_cycle(work_plan, available_agents)
            logger.info(f"Final result: {final_result}")

            # 4. ìµœì¢… ê²°ê³¼ ì •ë¦¬ ë° ë°˜í™˜
            user_friendly_result = self.format_final_result(work_plan, final_result)
            
            # ê²°ê³¼ê°€ ë¹„ì–´ìˆëŠ” ê²½ìš° ë°±ì—… ë©”ì‹œì§€ ì œê³µ
            if not user_friendly_result or len(user_friendly_result.strip()) == 0:
                user_friendly_result = f"A2A ì—ì´ì „íŠ¸ ì‹¤í–‰ ì™„ë£Œ!\n\nì‘ì—… ê²°ê³¼:\n{json.dumps(final_result, ensure_ascii=False, indent=2)}"
                logger.warning("Empty result detected, using backup message")
            
            return Message(
                text=user_friendly_result,
                sender=MESSAGE_SENDER_AI,
                sender_name=MESSAGE_SENDER_NAME_AI
            )

        except Exception as e:
            logger.error(f"Error in A2A agent execution: {e}")
            return Message(
                text=f"A2A ì—ì´ì „íŠ¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                sender=MESSAGE_SENDER_AI,
                sender_name=MESSAGE_SENDER_NAME_AI
            )



    def _extract_text_from_agent_response(self, agent_response: Dict[str, Any]) -> str:
        """A2A ì—ì´ì „íŠ¸ ì‘ë‹µì—ì„œ ì‹¤ì œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ëŠ” í—¬í¼ í•¨ìˆ˜"""
        try:
            # 1. í‘œì¤€ A2A ì‘ë‹µ êµ¬ì¡° í™•ì¸
            if isinstance(agent_response, dict) and "result" in agent_response:
                response_content = agent_response["result"]
                if isinstance(response_content, dict) and "parts" in response_content:
                    for part in response_content["parts"]:
                        if part.get("kind") == "text":
                            return part.get("text", "")
            
            # 2. Langflow ì‘ë‹µ êµ¬ì¡° í™•ì¸ (outputs -> results -> message -> text)
            if isinstance(agent_response, dict) and "outputs" in agent_response:
                outputs = agent_response["outputs"]
                if isinstance(outputs, list) and outputs:
                    first_output = outputs[0]
                    if isinstance(first_output, dict) and "results" in first_output:
                        results = first_output["results"]
                        if isinstance(results, dict) and "message" in results:
                            message = results["message"]
                            if isinstance(message, dict) and "text" in message:
                                return message["text"]
            
            # 3. ì¤‘ì²©ëœ outputs êµ¬ì¡° í™•ì¸
            if isinstance(agent_response, dict):
                # ì¬ê·€ì ìœ¼ë¡œ í…ìŠ¤íŠ¸ ì°¾ê¸°
                def find_text_recursive(obj, max_depth=5):
                    if max_depth <= 0:
                        return None
                    
                    if isinstance(obj, dict):
                        # text í‚¤ê°€ ìˆìœ¼ë©´ ë°˜í™˜
                        if "text" in obj and isinstance(obj["text"], str):
                            return obj["text"]
                        # ì¬ê·€ì ìœ¼ë¡œ ê²€ìƒ‰
                        for value in obj.values():
                            result = find_text_recursive(value, max_depth - 1)
                            if result:
                                return result
                    elif isinstance(obj, list):
                        for item in obj:
                            result = find_text_recursive(item, max_depth - 1)
                            if result:
                                return result
                    return None
                
                found_text = find_text_recursive(agent_response)
                if found_text:
                    return found_text
            
            # 4. JSON ë¬¸ìì—´ë¡œ ë³€í™˜í•´ì„œ í…ìŠ¤íŠ¸ ì°¾ê¸°
            import json
            json_str = json.dumps(agent_response, ensure_ascii=False)
            # ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ íŒ¨í„´ ì°¾ê¸°
            import re
            text_patterns = [
                r'"text":\s*"([^"]+)"',
                r'"message":\s*"([^"]+)"',
                r'"content":\s*"([^"]+)"'
            ]
            
            for pattern in text_patterns:
                matches = re.findall(pattern, json_str)
                if matches:
                    # ê°€ì¥ ê¸´ í…ìŠ¤íŠ¸ ë°˜í™˜
                    return max(matches, key=len)
            
            return ""
            
        except Exception as e:
            logger.error(f"Error extracting text from agent response: {e}")
            return ""

    def _extract_user_text_from_task(self, user_task: str) -> str:
        """ì‚¬ìš©ì ìš”ì²­ì—ì„œ ì‹¤ì œ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œí•˜ëŠ” í—¬í¼ í•¨ìˆ˜"""
        try:
            import json
            # JSON í˜•íƒœì¸ì§€ í™•ì¸
            if user_task.strip().startswith('{'):
                task_data = json.loads(user_task)
                if isinstance(task_data, dict) and "text" in task_data:
                    return task_data["text"]
            return user_task
        except Exception:
            return user_task

    def get_default_work_plan(self, user_task: str, available_agents: List[Dict[str, Any]]) -> Dict[str, Any]:
        """LLMì´ ì—†ì„ ë•Œ ì‚¬ìš©í•  ê¸°ë³¸ ì‘ì—… ê³„íš ìƒì„±"""
        logger.info("Generating default work plan")

        # ì‚¬ìš©ì ì…ë ¥ì„ ë¶„ì„í•˜ì—¬ ì‘ì—… ìœ í˜• ê²°ì •
        task_type = self.analyze_task_type(user_task)
        task_title = self.generate_task_title(user_task, task_type)
        
        # ê¸°ë³¸ ì‘ì—… ê³„íš ìƒì„±
        work_plan = {
            "goal": f"ì‚¬ìš©ì ìš”ì²­ ì²˜ë¦¬: {user_task}",
            "assumptions": [
                f"ì‚¬ìš©ìê°€ {task_type} ì‘ì—…ì„ ìš”ì²­í–ˆìŠµë‹ˆë‹¤.",
                "ë‚´ì¥ ê¸°ëŠ¥ìœ¼ë¡œ ì²˜ë¦¬ ê°€ëŠ¥í•©ë‹ˆë‹¤."
            ],
            "work_breakdown": [
                {
                    "id": "T1",
                    "title": task_title,
                    "agent": "self",
                    "skills": [task_type],
                    "inputs": [user_task],
                    "outputs": [f"{task_type} ê²°ê³¼"],
                    "dod": ["ì‘ì—…ì´ ì™„ë£Œë¨"],
                    "est_hours": 0.1,
                    "dependencies": [],
                    "parallelizable": False,
                    "risk": ["ê¸°ë³¸ ì²˜ë¦¬"],
                    "mitigation": ["í‘œì¤€ ì ˆì°¨ ì‚¬ìš©"],
                    "bus": {
                        "use_bus": False,
                        "topics": [],
                        "role": "consumer",
                        "message_schema": {},
                        "qos": {},
                        "contracts": []
                    }
                }
            ],
            "critical_path": ["T1"],
            "execution_plan": [
                {
                    "phase": "Execution",
                    "targets": ["T1"],
                    "notes": f"{task_type} ì‘ì—…ì„ ì§ì ‘ ì‹¤í–‰í•©ë‹ˆë‹¤."
                }
            ],
            "risks_global": ["ê¸°ë³¸ ì²˜ë¦¬ ì œí•œ"],
            "mitigations_global": ["í•„ìš”ì‹œ ìˆ˜ë™ ê°œì…"]
        }

        logger.info(f"Default work plan created with {len(work_plan['work_breakdown'])} tasks")
        return work_plan

    def analyze_task_type(self, user_task: str) -> str:
        """ì‚¬ìš©ì ì…ë ¥ì„ ë¶„ì„í•˜ì—¬ ì‘ì—… ìœ í˜•ì„ ê²°ì •"""
        user_task_lower = user_task.lower()
        
        if any(keyword in user_task_lower for keyword in ["ë¶„ì„", "ë‹¨ì–´", "ë¬¸ì", "ê°œìˆ˜", "í†µê³„"]):
            return "í…ìŠ¤íŠ¸ ë¶„ì„"
        elif any(keyword in user_task_lower for keyword in ["ì§ˆë¬¸", "ë‹µë³€", "ë¬¼ì–´", "ì•Œë ¤", "ì„¤ëª…", "?", "ë¬´ì—‡", "ì–´ë–»ê²Œ", "ì™œ"]):
            return "ì§ˆì˜ì‘ë‹µ"
        elif any(keyword in user_task_lower for keyword in ["ìš”ì•½", "ì •ë¦¬", "ê°„ì¶”", "í•µì‹¬"]):
            return "ìš”ì•½"
        elif any(keyword in user_task_lower for keyword in ["ìƒì„±", "ë§Œë“¤", "ì‘ì„±", "ì°½ì‘", "ì“°ê¸°"]):
            return "ìƒì„±"
        elif any(keyword in user_task_lower for keyword in ["ë²ˆì—­", "translate"]):
            return "ë²ˆì—­"
        elif any(keyword in user_task_lower for keyword in ["ê³„ì‚°", "ìˆ˜í•™", "ê³±ì…ˆ", "ë‚˜ëˆ—ì…ˆ", "ë”í•˜ê¸°", "ë¹¼ê¸°"]):
            return "ê³„ì‚°"
        else:
            return "ì¼ë°˜ ì‘ì—…"

    def generate_task_title(self, user_task: str, task_type: str) -> str:
        """ì‘ì—… ìœ í˜•ì— ë”°ë¥¸ ì ì ˆí•œ ì œëª© ìƒì„±"""
        if task_type == "í…ìŠ¤íŠ¸ ë¶„ì„":
            return "í…ìŠ¤íŠ¸ ë¶„ì„ ë° í†µê³„ ì¶”ì¶œ"
        elif task_type == "ì§ˆì˜ì‘ë‹µ":
            return "ì§ˆë¬¸ ì‘ë‹µ ì²˜ë¦¬"
        elif task_type == "ìš”ì•½":
            return "í…ìŠ¤íŠ¸ ìš”ì•½ ìƒì„±"
        elif task_type == "ìƒì„±":
            return "ì½˜í…ì¸  ìƒì„±"
        elif task_type == "ë²ˆì—­":
            return "ì–¸ì–´ ë²ˆì—­"
        elif task_type == "ê³„ì‚°":
            return "ìˆ˜í•™ ê³„ì‚° ìˆ˜í–‰"
        else:
            return "ì¼ë°˜ ì‘ì—… ì²˜ë¦¬"

    def _log_work_breakdown(self, work_plan: Dict[str, Any]) -> None:
        """ì‘ì—… ê³„íšì˜ ì‘ì—… ëª©ë¡ì„ ë¡œê·¸ë¡œ ì¶œë ¥í•˜ëŠ” í—¬í¼ í•¨ìˆ˜"""
        if "work_breakdown" in work_plan:
            tasks = work_plan["work_breakdown"]
            logger.info(f"ìˆ˜ë¦½ëœ ì‘ì—… ëª©ë¡ ({len(tasks)}ê°œ):")
            for i, task in enumerate(tasks, 1):
                title = task.get("title", f"Task {task.get('id', 'Unknown')}")
                outputs = task.get("outputs", [])
                dod = task.get("dod", [])
                logger.info(f"  {i}. {title}\r\n     ì¶œë ¥: {', '.join(outputs)}\r\n     ì™„ë£Œ ê¸°ì¤€: {', '.join(dod)}")



    def format_final_result(self, work_plan: Dict[str, Any], final_result: Dict[str, Any]) -> str:
        """ìµœì¢… ê²°ê³¼ë¥¼ ì‚¬ìš©ì ì¹œí™”ì ì¸ í˜•íƒœë¡œ í¬ë§·íŒ…"""
        
        try:
            logger.info("Starting format_final_result")
            logger.info(f"work_plan keys: {list(work_plan.keys()) if work_plan else 'None'}")
            logger.info(f"final_result keys: {list(final_result.keys()) if final_result else 'None'}")
            
            # í—¤ë”
            result_text = "ğŸ¯ **A2A ì—ì´ì „íŠ¸ ì‹¤í–‰ ê²°ê³¼**\n\n"
            
            # ì‘ì—… ê°œìš”
            tasks = work_plan.get("work_breakdown", [])
            execution_results = final_result.get("execution_results", {})
            iterations = final_result.get("iterations", 0)
            
            result_text += f"ğŸ“‹ **ì‘ì—… ê°œìš”**\n"
            result_text += f"â€¢ ì´ ì‘ì—… ìˆ˜: {len(tasks)}ê°œ\n"
            result_text += f"â€¢ ì™„ë£Œëœ ì‘ì—…: {len(execution_results)}ê°œ\n"
            result_text += f"â€¢ ì‹¤í–‰ ë°˜ë³µ: {iterations + 1}íšŒ\n\n"
            
            # ê° ì‘ì—…ë³„ ê²°ê³¼
            result_text += "ğŸ“Š **ì‘ì—…ë³„ ì‹¤í–‰ ê²°ê³¼**\n\n"
            
            for i, task in enumerate(tasks, 1):
                task_id = task["id"]
                task_title = task["title"]
                task_agent = task["agent"]
                
                result_text += f"**{i}. {task_title}**\n"
                result_text += f"   â€¢ ë‹´ë‹¹: {task_agent}\n"
                
                if task_id in execution_results:
                    task_result = execution_results[task_id]
                    status = task_result.get("status", "unknown")
                    
                    if status == "completed":
                        result_text += f"   â€¢ ìƒíƒœ: âœ… ì™„ë£Œ\n"
                        
                        # ê²°ê³¼ ë‚´ìš© ì¶”ì¶œ
                        result_data = task_result.get("result", {})
                        
                        # ìƒˆë¡œìš´ ì§ì ‘ ì‹¤í–‰ ê²°ê³¼ ì²˜ë¦¬
                        if isinstance(result_data, dict):
                            if "execution_method" in result_data:
                                execution_method = result_data["execution_method"]
                                result_content = result_data.get("result", "")
                                
                                if execution_method == "text_analysis":
                                    # í…ìŠ¤íŠ¸ ë¶„ì„ ê²°ê³¼
                                    analysis = result_data.get("analysis", {})
                                    result_text += f"   â€¢ ì‹¤í–‰ ë°©ë²•: ğŸ“Š í…ìŠ¤íŠ¸ ë¶„ì„\n"
                                    result_text += f"   â€¢ ë‹¨ì–´ ìˆ˜: {analysis.get('ë‹¨ì–´_ìˆ˜', 'N/A')}ê°œ\n"
                                    result_text += f"   â€¢ ë¬¸ì ìˆ˜: {analysis.get('ë¬¸ì_ìˆ˜_ê³µë°±í¬í•¨', 'N/A')}ê°œ\n"
                                    result_text += f"   â€¢ ë¬¸ì¥ ìˆ˜: {analysis.get('ë¬¸ì¥_ìˆ˜', 'N/A')}ê°œ\n"
                                elif execution_method in ["llm_qa", "default_qa"]:
                                    # ì§ˆì˜ì‘ë‹µ ê²°ê³¼
                                    question = result_data.get("question", "")
                                    answer = result_data.get("answer", "")
                                    result_text += f"   â€¢ ì‹¤í–‰ ë°©ë²•: â“ ì§ˆì˜ì‘ë‹µ\n"
                                    result_text += f"   â€¢ ì§ˆë¬¸: {question[:50]}{'...' if len(question) > 50 else ''}\n"
                                    result_text += f"   â€¢ ë‹µë³€: {answer[:100]}{'...' if len(answer) > 100 else ''}\n"
                                elif execution_method in ["llm_summary", "basic_summary"]:
                                    # ìš”ì•½ ê²°ê³¼
                                    original_length = len(result_data.get("original_text", ""))
                                    summary = result_data.get("summary", "")
                                    result_text += f"   â€¢ ì‹¤í–‰ ë°©ë²•: ğŸ“ ìš”ì•½\n"
                                    result_text += f"   â€¢ ì›ë³¸ ê¸¸ì´: {original_length}ì\n"
                                    result_text += f"   â€¢ ìš”ì•½: {summary[:100]}{'...' if len(summary) > 100 else ''}\n"
                                elif execution_method in ["llm_generation", "default_generation"]:
                                    # ìƒì„± ê²°ê³¼
                                    request = result_data.get("request", "")
                                    generated = result_data.get("generated_content", "")
                                    result_text += f"   â€¢ ì‹¤í–‰ ë°©ë²•: âœ¨ ìƒì„±\n"
                                    result_text += f"   â€¢ ìš”ì²­: {request[:50]}{'...' if len(request) > 50 else ''}\n"
                                    result_text += f"   â€¢ ìƒì„± ê²°ê³¼: {generated[:100]}{'...' if len(generated) > 100 else ''}\n"
                                elif execution_method in ["llm_general", "basic_general"]:
                                    # ì¼ë°˜ ì‘ì—… ê²°ê³¼
                                    task_desc = result_data.get("task_description", "")
                                    user_inp = result_data.get("user_input", "")
                                    result_text += f"   â€¢ ì‹¤í–‰ ë°©ë²•: ğŸ”§ ì¼ë°˜ ì‘ì—…\n"
                                    result_text += f"   â€¢ ì‘ì—…: {task_desc}\n"
                                    result_text += f"   â€¢ ì…ë ¥: {user_inp[:50]}{'...' if len(user_inp) > 50 else ''}\n"
                                elif execution_method == "direct_calculation":
                                    # ì§ì ‘ ê³„ì‚° ê²°ê³¼
                                    calc_result = result_data.get("result", "")
                                    inputs = result_data.get("inputs", {})
                                    calculation = result_data.get("calculation", {})
                                    
                                    result_text += f"   â€¢ ì‹¤í–‰ ë°©ë²•: ğŸ§® ìˆ˜í•™ ê³„ì‚°\n"
                                    result_text += f"   â€¢ ê²°ê³¼: {calc_result}\n"
                                    if inputs:
                                        result_text += f"   â€¢ ì…ë ¥ê°’: ë‹¨ì–´ìˆ˜ {inputs.get('word_count', 'N/A')}, í† í°ìˆ˜ {inputs.get('token_count', 'N/A')}\n"
                                else:
                                    # ê¸°íƒ€ ì§ì ‘ ì‹¤í–‰ ê²°ê³¼
                                    result_text += f"   â€¢ ì‹¤í–‰ ë°©ë²•: {execution_method}\n"
                                    if isinstance(result_content, str):
                                        lines = result_content.strip().split('\n')
                                        result_text += f"   â€¢ ê²°ê³¼:\n"
                                        for line in lines[:3]:  # ì²« 3ì¤„ë§Œ í‘œì‹œ
                                            result_text += f"     {line.strip()}\n"
                                        if len(lines) > 3:
                                            result_text += "     ...\n"
                                            
                            elif "agent_response" in result_data:
                                # A2A ì—ì´ì „íŠ¸ ì‘ë‹µ ê°œì„ ëœ íŒŒì‹±
                                agent_response = result_data["agent_response"]
                                extracted_text = self._extract_text_from_agent_response(agent_response)
                                
                                if extracted_text:
                                    # í…ìŠ¤íŠ¸ë¥¼ ì¤„ ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬
                                    lines = extracted_text.strip().split('\n')
                                    result_text += f"   â€¢ ì‹¤í–‰ ë°©ë²•: ğŸ¤– ì™¸ë¶€ ì—ì´ì „íŠ¸\n"
                                    result_text += f"   â€¢ ê²°ê³¼:\n"
                                    for line in lines[:5]:  # ì²« 5ì¤„ í‘œì‹œ
                                        if line.strip():
                                            result_text += f"     {line.strip()}\n"
                                    if len(lines) > 5:
                                        result_text += "     ...\n"
                                else:
                                    result_text += f"   â€¢ ì‹¤í–‰ ë°©ë²•: ğŸ¤– ì™¸ë¶€ ì—ì´ì „íŠ¸\n"
                                    result_text += f"   â€¢ ê²°ê³¼: ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨\n"
                        elif isinstance(result_data, str):
                            # ë‹¨ìˆœ ë¬¸ìì—´ ê²°ê³¼
                            lines = result_data.strip().split('\n')
                            result_text += f"   â€¢ ê²°ê³¼:\n"
                            for line in lines[:3]:  # ì²« 3ì¤„ë§Œ í‘œì‹œ
                                result_text += f"     {line.strip()}\n"
                            if len(lines) > 3:
                                result_text += "     ...\n"
                    elif status == "failed":
                        result_text += f"   â€¢ ìƒíƒœ: âŒ ì‹¤íŒ¨\n"
                        error = task_result.get("error", "ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜")
                        result_text += f"   â€¢ ì˜¤ë¥˜: {error}\n"
                    elif status == "skipped":
                        result_text += f"   â€¢ ìƒíƒœ: â­ï¸ ê±´ë„ˆëœ€\n"
                        reason = task_result.get("reason", "")
                        result_text += f"   â€¢ ì‚¬ìœ : {reason}\n"
                else:
                    result_text += f"   â€¢ ìƒíƒœ: â¸ï¸ ë¯¸ì‹¤í–‰\n"
                
                result_text += "\n"
            
            # ìµœì¢… ìš”ì•½
            completed_count = sum(1 for result in execution_results.values() if result.get("status") == "completed")
            failed_count = sum(1 for result in execution_results.values() if result.get("status") == "failed")
            
            result_text += "ğŸ **ìµœì¢… ìš”ì•½**\n"
            if completed_count == len(tasks) and failed_count == 0:
                result_text += "âœ… ëª¨ë“  ì‘ì—…ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!\n"
            elif failed_count > 0:
                result_text += f"âš ï¸ {failed_count}ê°œ ì‘ì—…ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ({completed_count}/{len(tasks)} ì™„ë£Œ)\n"
            else:
                result_text += f"ğŸ”„ ì‘ì—…ì´ ì§„í–‰ ì¤‘ì…ë‹ˆë‹¤. ({completed_count}/{len(tasks)} ì™„ë£Œ)\n"
            
            # ì‚¬ìš©ì ìš”ì²­ ì›ë¬¸ë„ í¬í•¨ (ê¹”ë”í•˜ê²Œ ì •ë¦¬)
            if hasattr(self, 'user_task') and self.user_task:
                # JSON í˜•íƒœì˜ user_taskë¥¼ íŒŒì‹±í•´ì„œ ì‹¤ì œ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
                user_text = self._extract_user_text_from_task(self.user_task)
                if user_text:
                    result_text += f"\nğŸ“ **ì›ë³¸ ìš”ì²­**: {user_text}\n"
                else:
                    result_text += f"\nğŸ“ **ì›ë³¸ ìš”ì²­**: {self.user_task}\n"
            
            logger.info(f"format_final_result completed, length: {len(result_text)}")
            return result_text
            
        except Exception as e:
            logger.error(f"Error in format_final_result: {e}")
            return f"A2A ì—ì´ì „íŠ¸ ì‹¤í–‰ ì™„ë£Œ!\n\nì‘ì—… ì™„ë£Œ: {len(final_result.get('execution_results', {}))}ê°œ\nìƒíƒœ: {final_result.get('status', 'unknown')}\n\nìƒì„¸ ê²°ê³¼ëŠ” ë¡œê·¸ë¥¼ í™•ì¸í•˜ì„¸ìš”."

    async def get_available_agents(self) -> List[Dict[str, Any]]:
        """A2A Discovery ì„œë¹„ìŠ¤ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ì—ì´ì „íŠ¸ ëª©ë¡ì„ ì¡°íšŒ"""
        try:
            api_url = f"{self.a2a_api_base}/agents?status=active"
            
            async with aiohttp.ClientSession() as session:
                async with session.get(api_url) as response:
                    if response.status == 200:
                        response_data = await response.json()
                       
                        # A2A Discovery ì„œë¹„ìŠ¤ëŠ” AgentListResponse í˜•íƒœë¡œ ì‘ë‹µ: {"agents": [...]}
                        if isinstance(response_data, dict) and 'agents' in response_data:
                            agents = response_data['agents']
                            logger.info(f"A2A Discovery : Found {len(agents)} available agents.")
                            return agents
                        else:
                            logger.warning(f"A2A Discovery : Unexpected response format from A2A Discovery service: {type(response_data)}")
                            return []
                    else:
                        logger.warning(f"A2A Discovery : Failed to get agents from A2A Discovery service: {response.status}")
                        return []
        except Exception as e:
            logger.warning(f"A2A Discovery : Error connecting to A2A Discovery service: {e}")
            return []
        

    async def create_work_plan(self, available_agents: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ì‚¬ìš©ì ì‘ì—…ì„ ë¶„ì„í•˜ê³  ì‘ì—… ë¶„í•  ê³„íšì„ ìˆ˜ë¦½"""
        
        # Work Plan Prompt í•„ìˆ˜ ì—°ê²° í™•ì¸
        if not hasattr(self, 'work_plan_prompt') or not self.work_plan_prompt:
            msg = "Work Plan Promptê°€ ì—°ê²°ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. Prompt Template ì»´í¬ë„ŒíŠ¸ë¥¼ ì—°ê²°í•´ì£¼ì„¸ìš”."
            logger.error(msg)
            raise ValueError(msg)
                
        planning_prompt = create_prompt_from_component(
            self.work_plan_prompt,
            default_system_message=self.system_prompt
        )
        
        if planning_prompt is None:
            msg = "Work Plan Prompt ë³€í™˜ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì˜¬ë°”ë¥¸ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì„ ì—°ê²°í•´ì£¼ì„¸ìš”."
            logger.error(msg)
            raise ValueError(msg)

        # OUTPUT_SCHEMA ì •ì˜
        output_schema = {
            "type": "object",
            "properties": {
                "goal": {"type": "string", "description": "ì „ì²´ ëª©í‘œ"},
                "assumptions": {"type": "array", "items": {"type": "string"}, "description": "ê°€ì •ì‚¬í•­ë“¤"},
                "updated": {"type": "boolean", "description": "ê³„íšì´ ì—…ë°ì´íŠ¸ë˜ì—ˆëŠ”ì§€ ì—¬ë¶€"},
                "work_breakdown": {
                    "type": "array",
                    "items": {
                        "type": "object",
                        "properties": {
                            "id": {"type": "string", "description": "íƒœìŠ¤í¬ ID (ì˜ˆ: T1, T2)"},
                            "title": {"type": "string", "description": "íƒœìŠ¤í¬ ì œëª©"},
                            "agent": {"type": "string", "description": "ë‹´ë‹¹ ì—ì´ì „íŠ¸ ID ë˜ëŠ” ì´ë¦„"},
                            "skills": {"type": "array", "items": {"type": "string"}, "description": "í•„ìš”í•œ ìŠ¤í‚¬ë“¤"},
                            "inputs": {"type": "array", "items": {"type": "string"}, "description": "ì…ë ¥ í•­ëª©ë“¤"},
                            "outputs": {"type": "array", "items": {"type": "string"}, "description": "ì¶œë ¥ í•­ëª©ë“¤"},
                            "dod": {"type": "array", "items": {"type": "string"}, "description": "ì™„ë£Œ ê¸°ì¤€(Definition of Done)"},
                            "est_hours": {"type": "number", "description": "ì¶”ì • ì†Œìš” ì‹œê°„(ì‹œê°„ ë‹¨ìœ„)"},
                            "dependencies": {"type": "array", "items": {"type": "string"}, "description": "ì˜ì¡´ì„± íƒœìŠ¤í¬ IDë“¤"},
                            "parallelizable": {"type": "boolean", "description": "ë³‘ë ¬ ì‹¤í–‰ ê°€ëŠ¥ ì—¬ë¶€"},
                            "risk": {"type": "array", "items": {"type": "string"}, "description": "ë¦¬ìŠ¤í¬ ìš”ì†Œë“¤"},
                            "mitigation": {"type": "array", "items": {"type": "string"}, "description": "ë¦¬ìŠ¤í¬ ì™„í™” ë°©ì•ˆë“¤"},
                            "bus": {
                                "type": "object",
                                "properties": {
                                    "use_bus": {"type": "boolean"},
                                    "topics": {"type": "array", "items": {"type": "string"}},
                                    "role": {"type": "string", "enum": ["producer", "consumer", "both"]},
                                    "message_schema": {"type": "object"},
                                    "qos": {"type": "object"},
                                    "contracts": {"type": "array", "items": {"type": "string"}}
                                }
                            }
                        }
                    }
                },
                "critical_path": {"type": "array", "items": {"type": "string"}, "description": "í¬ë¦¬í‹°ì»¬ íŒ¨ìŠ¤ íƒœìŠ¤í¬ IDë“¤"},
                "execution_plan": {
                    "type": "array",
                    "items": {
                        "type": "object",
                        "properties": {
                            "phase": {"type": "string", "description": "ì‹¤í–‰ ë‹¨ê³„"},
                            "targets": {"type": "array", "items": {"type": "string"}, "description": "í•´ë‹¹ ë‹¨ê³„ì˜ íƒœìŠ¤í¬ IDë“¤"},
                            "notes": {"type": "string", "description": "ë‹¨ê³„ë³„ ì°¸ê³ ì‚¬í•­"}
                        }
                    }
                },
                "risks_global": {"type": "array", "items": {"type": "string"}, "description": "ì „ì²´ í”„ë¡œì íŠ¸ ë¦¬ìŠ¤í¬"},
                "mitigations_global": {"type": "array", "items": {"type": "string"}, "description": "ì „ì²´ í”„ë¡œì íŠ¸ ë¦¬ìŠ¤í¬ ì™„í™” ë°©ì•ˆ"}
            }
        }

        # LangChainì´ ì´í•´í•  ìˆ˜ ìˆëŠ” í˜•ì‹ìœ¼ë¡œ ë©”ì‹œì§€ ë³€í™˜
        user_task = self.input_value.content if hasattr(self.input_value, 'content') else str(self.input_value)
        self.user_task = user_task  # ì¸ìŠ¤í„´ìŠ¤ ë³€ìˆ˜ë¡œ ì €ì¥
        
        # LLMì´ ìˆëŠ”ì§€ í™•ì¸í•˜ê³  ì—†ìœ¼ë©´ ê¸°ë³¸ ê³„íš ë°˜í™˜
        if not hasattr(self, 'llm') or self.llm is None:
            logger.warning("LLM not available, using default work plan")
            return self.get_default_work_plan(user_task, available_agents)

        # LLMì„ ì‚¬ìš©í•˜ì—¬ ì‘ì—… ê³„íš ìˆ˜ë¦½
        chain = planning_prompt | self.llm | JsonOutputParser()
        
        try:
            
            # chat_historyë¥¼ LangChain í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            langchain_chat_history = []
            if self.chat_history:
                for msg in self.chat_history:
                    if hasattr(msg, 'content') and hasattr(msg, 'type'):
                        if msg.type == "human":
                            langchain_chat_history.append(HumanMessage(content=msg.content))
                        elif msg.type == "ai":
                            langchain_chat_history.append(AIMessage(content=msg.content))
                        else:
                            langchain_chat_history.append(HumanMessage(content=str(msg.content)))

            result = await chain.ainvoke({
                "user_task": user_task,
                "available_agents": json.dumps(available_agents, ensure_ascii=False, indent=2),
                "output_schema": json.dumps(output_schema, ensure_ascii=False, indent=2),
                "chat_history": langchain_chat_history
            })
            
            logger.info(f"Work plan created with {len(result.get('work_breakdown', []))} tasks")

            # ìˆ˜ë¦½ëœ ì‘ì—… ì œëª©ë“¤ ì¶œë ¥
            self._log_work_breakdown(result)
            
            return result
            
        except Exception as e:
            logger.error(f"Error creating work plan: {e}")
            raise

    async def execute_planning_dispatcher_critic_cycle(self, work_plan: Dict[str, Any], available_agents: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """Planning-Dispatcher-Critic ì‚¬ì´í´ì„ ì‹¤í–‰í•˜ì—¬ ì‘ì—…ì„ ìˆ˜í–‰"""
        
        current_plan = work_plan
        iteration = 0
        execution_results = {}  # ì´ˆê¸°í™”
        
        # A2A ì „ìš© ë°˜ë³µ íšŸìˆ˜
        max_iter = self.a2a_max_iterations
        logger.info(f"Starting PDC cycle with A2A max_iterations: {max_iter}")
        
        while iteration < max_iter:
            logger.info(f"Starting iteration {iteration + 1} of {max_iter}")
            
            try:
                # 1. Dispatcher: ì‘ì—…ì„ ì ì ˆí•œ ì—ì´ì „íŠ¸ì—ê²Œ ìœ„ì„
                task_id,new_results = await self.dispatcher_phase(current_plan, available_agents, execution_results)
                execution_results.update(new_results)

                # 2. Critic: ê²°ê³¼ ë¶„ì„ ë° ì™„ë£Œ ì—¬ë¶€ íŒë‹¨
                critique_result = await self.critic_phase(task_id, execution_results, current_plan)
                logger.info(f"Critic phase completed. Analysis: {critique_result.get('analysis', 'N/A')}")
                execution_results[task_id]["result"]["output_values"] = critique_result["output_values"]
                
                # 3. ë‹¤ìŒ ë°˜ë³µì„ ìœ„í•œ ê³„íš ìœ ì§€ (Planning Phase ì œê±°ë¨)
                # current_planì€ ê·¸ëŒ€ë¡œ ìœ ì§€
                logger.info(f"Plan set for next iteration: {len(current_plan.get('work_breakdown', []))} tasks")
                self._log_work_breakdown(current_plan)

                # 4. ì™„ë£Œ ì¡°ê±´ í™•ì¸
                if critique_result.get("is_complete", False):
                    logger.info(f"Task completed after {iteration + 1} iterations")
                    break
                    
                iteration += 1
                
                # 6. ì ì‹œ ëŒ€ê¸° (ê³¼ë„í•œ API í˜¸ì¶œ ë°©ì§€)
                await asyncio.sleep(1)
                
            except Exception as e:
                logger.error(f"Error in iteration {iteration + 1}: {e}")
                break
        
        logger.info(f"PDC cycle completed after {iteration} iterations (max: {max_iter})")
        
        return {
            # "final_plan": current_plan,
            "execution_results": execution_results,
            "iterations": iteration,
            "status": "completed" if iteration < max_iter else "max_iterations_reached"
        }

    async def dispatcher_phase(self, plan: Dict[str, Any], available_agents: List[Dict[str, Any]], existing_results: Dict[str, Any] = None) -> Dict[str, Any]:
        """ë””ìŠ¤íŒ¨ì²˜ ë‹¨ê³„: ì‘ì—…ì„ ì ì ˆí•œ ì—ì´ì „íŠ¸ì—ê²Œ ìœ„ì„"""
        
        logger.info("Starting dispatcher phase...")

        # ê¸°ì¡´ ê²°ê³¼ê°€ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ë¹ˆ ë”•ì…”ë„ˆë¦¬ë¡œ ì‹œì‘
        if existing_results is None:
            existing_results = {}
        
        new_execution_results = {}
        
        # ë””ë²„ê¹…: í˜„ì¬ ìƒíƒœ ë¡œê·¸
        all_tasks = plan.get("work_breakdown", [])
        logger.info(f"Dispatcher: ì´ {len(all_tasks)}ê°œ ì‘ì—…, ê¸°ì¡´ ê²°ê³¼: {list(existing_results.keys()) if existing_results else 'ì—†ìŒ'}")
        
        # ì•„ì§ ì²˜ë¦¬ë˜ì§€ ì•Šì€ ì²« ë²ˆì§¸ ì‘ì—…ë§Œ ì²˜ë¦¬
        for task in all_tasks:
            task_id = task["id"]
            agent_id = task["agent"]
            
            logger.info(f"Dispatcher: ì‘ì—… {task_id} (ì—ì´ì „íŠ¸: {agent_id})")
            
            # ì´ë¯¸ ì²˜ë¦¬ëœ ì‘ì—…ì€ ê±´ë„ˆë›°ê¸°
            if task_id in existing_results:
                logger.info(f"Task {task_id} already processed, skipping")
                continue
                
            
            # ì´ì „ ì‘ì—… ê²°ê³¼ì—ì„œ í˜„ì¬ ì‘ì—…ì˜ í•„ìš” ì¶œë ¥ì´ ì´ë¯¸ ìˆëŠ”ì§€ í™•ì¸
            required_outputs = task.get("outputs", [])

            if required_outputs and existing_results:

                #ì´ë²ˆ task ì‚°ì¶œë¬¼ì´ ì´ì „ ì‘ì—…ì—ì„œ ì´ë¯¸ ë„ì¶œ ëœ ê²½ìš°, ì´ë²ˆ taskì™„ë£Œëœ ê²ƒìœ¼ë¡œ ì²˜ë¦¬í•˜ê³  critic ë‹¨ê³„ë¥¼ ìˆ˜í–‰
                task_outputs = await self.check_task_outputs_in_existing_results(task["outputs"], existing_results);
                if task_outputs:  
                    if task_outputs.get("found"):
                        logger.info(f"Task {task_id} ìŠ¤í‚µ: í•„ìš”í•œ ì‚°ì¶œë¬¼ì´ ì´ë¯¸ ì´ì „ ì‘ì—…ì—ì„œ ìƒì„±ë¨")
                        new_execution_results[task_id] = {
                            "status": "skipped",
                            "result": task_outputs,
                            "agent": "previous_results",
                            "reason": task_outputs.get("message", "Required outputs already available")
                        }
                        break   
                    
            try:
                # ì—ì´ì „íŠ¸ì—ê²Œ ì‘ì—… ìœ„ì„
                if agent_id != "self":
                    result = await self.delegate_task_to_agent(task, agent_id, available_agents)
                else:
                    # ì§ì ‘ ì‹¤í–‰
                    result = await self.execute_task_directly(task, existing_results)
                
                new_execution_results[task_id] = {
                    "status": "completed",
                    "result": result,
                    "agent": agent_id
                }
                
                # í•œ ë²ˆì— í•˜ë‚˜ì˜ ì‘ì—…ë§Œ ì²˜ë¦¬í•˜ê³  ë°˜í™˜ (PDC ì‚¬ì´í´ì„ ìœ„í•´)
                break
                
            except Exception as e:
                logger.error(f"Task {task_id} execution failed: {e}")
                new_execution_results[task_id] = {
                    "status": "failed",
                    "error": str(e),
                    "agent": agent_id
                }
                # ì‹¤íŒ¨í•œ ê²½ìš°ì—ë„ í•œ ë²ˆì— í•˜ë‚˜ë§Œ ì²˜ë¦¬
                break
        
        # ëª¨ë“  ì‘ì—…ì„ í™•ì¸í–ˆì§€ë§Œ ì²˜ë¦¬í•  ê²ƒì´ ì—†ëŠ” ê²½ìš°
        if len(new_execution_results) == 0:
            logger.info(f"Dispatcher: ì²˜ë¦¬í•  ìƒˆë¡œìš´ ì‘ì—…ì´ ì—†ìŠµë‹ˆë‹¤. (ì´ {len(all_tasks)}ê°œ ì‘ì—… ì¤‘ {len(existing_results)}ê°œ ì™„ë£Œ)")
        
        logger.info(f"Dispatcher phase completed. Returning {len(new_execution_results)} new results")
        return task_id, new_execution_results



    async def check_task_outputs_in_existing_results(self, required_outputs: List[str], existing_results: Dict[str, Any]) -> Dict[str, Any]:
        """í˜„ì¬ ì‘ì—…ì˜ ì‚°ì¶œë¬¼ì´ ì´ì „ ì‘ì—… ê²°ê³¼ì— ì´ë¯¸ ìˆëŠ”ì§€ í™•ì¸"""
        
        if not required_outputs or not existing_results:
            return {}
        
        try:
            logger.info(f"--------------------------------")
            logger.info(f"check_task_outputs_in_existing_results: required_outputs: {required_outputs}")
            logger.info(f"check_task_outputs_in_existing_results: existing_results: {existing_results}")
            logger.info(f"--------------------------------")
            # LLMì„ ì´ìš©í•˜ì—¬ execution_results[task_id]["result"]["output_values"]ì— required_outputsê°€ ìˆëŠ”ì§€ í™•ì¸
            prompt = f"""ë‹¤ìŒ ì‚°ì¶œë¬¼ì´ ì´ì „ ì‘ì—… ê²°ê³¼ì— ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”: {required_outputs}

ì´ì „ ì‘ì—… ê²°ê³¼: {existing_results}

ë°˜ë“œì‹œ ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
{{
    "found": true/false,
    "message": "ë°œê²¬ë¨/ë°œê²¬ë˜ì§€ ì•ŠìŒ"
}}"""
            parser = JsonOutputParser()
            chain = self.llm | parser
            result = await chain.ainvoke([HumanMessage(content=prompt)])
            logger.info(f"check_task_outputs_in_existing_results: result: {result}")    
            return result
        except Exception as e:
            logger.error(f"check_task_outputs_in_existing_results ì‹¤í–‰ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")
            return {"error": str(e)}



    async def _get_available_tools(self) -> List[Dict[str, Any]]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ ëª©ë¡ì„ ë°˜í™˜ (HandleInputìœ¼ë¡œ ì—°ê²°ëœ ë„êµ¬ë§Œ)"""
        tools = []
        
        # HandleInputìœ¼ë¡œ ì—°ê²°ëœ ë„êµ¬ë“¤ë§Œ ì‚¬ìš©
        if hasattr(self, 'tools') and self.tools:
            try:
                # toolsê°€ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš°
                if isinstance(self.tools, list):
                    for tool in self.tools:
                        if hasattr(tool, 'name') and hasattr(tool, 'description'):
                            tools.append({
                                "name": tool.name,
                                "description": tool.description,
                                "capabilities": getattr(tool, 'capabilities', []),
                                "tool_object": tool  # ì‹¤ì œ ë„êµ¬ ê°ì²´ë„ ì €ì¥
                            })
                        elif isinstance(tool, dict):
                            tools.append({
                                "name": tool.get("name", "unknown_tool"),
                                "description": tool.get("description", "No description"),
                                "capabilities": tool.get("capabilities", []),
                                "tool_object": tool
                            })
                # toolsê°€ ë‹¨ì¼ ê°ì²´ì¸ ê²½ìš°
                elif hasattr(self.tools, 'name'):
                    tools.append({
                        "name": self.tools.name,
                        "description": self.tools.description,
                        "capabilities": getattr(self.tools, 'capabilities', []),
                        "tool_object": self.tools
                    })
                else:
                    logger.warning("ì—°ê²°ëœ ë„êµ¬ì˜ í˜•ì‹ì„ ì¸ì‹í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    
            except Exception as e:
                logger.warning(f"ë„êµ¬ ì •ë³´ íŒŒì‹± ì¤‘ ì˜¤ë¥˜: {e}")
        
        logger.info(f"ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬: {len(tools)}ê°œ")
        for tool in tools:
            logger.info(f"  - {tool['name']}: {tool['description']}")
        
        return tools

    async def _select_best_tool(self, task: Dict[str, Any], available_tools: List[Dict[str, Any]]) -> Dict[str, Any]:
        """LLMì„ ì‚¬ìš©í•˜ì—¬ ì‘ì—…ì— ê°€ì¥ ì í•©í•œ ë„êµ¬ë¥¼ ì„ íƒí•˜ê³  ì í•©ë„ë¥¼ í‰ê°€"""
        
        if not available_tools:
            logger.warning("ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return {
                "tool": None,
                "fitness_score": 0.0,
                "reasoning": "ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ê°€ ì—†ìŒ"
            }
        
        if not hasattr(self, 'llm') or self.llm is None:
            logger.warning("LLMì´ ì—†ì–´ ë„êµ¬ ì„ íƒì„ í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì²« ë²ˆì§¸ ë„êµ¬ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            return {
                "tool": available_tools[0],
                "fitness_score": 0.5,
                "reasoning": "LLM ì—†ìŒ - ì²« ë²ˆì§¸ ë„êµ¬ ì‚¬ìš©"
            }
        
        try:
            from langchain_core.messages import HumanMessage
            from langchain_core.output_parsers import JsonOutputParser
            
            # ë„êµ¬ ì„ íƒì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸
            tools_info = []
            for i, tool in enumerate(available_tools):
                tools_info.append(f"{i+1}. {tool['name']}: {tool['description']}")
                if tool.get('capabilities'):
                    tools_info.append(f"   ê¸°ëŠ¥: {', '.join(tool['capabilities'])}")
            
            prompt = f"""ë‹¤ìŒ ì‘ì—…ì— ê°€ì¥ ì í•©í•œ ë„êµ¬ë¥¼ ì„ íƒí•˜ê³  ì í•©ë„ë¥¼ 0-1 ì‚¬ì´ë¡œ í‰ê°€í•´ì£¼ì„¸ìš”.

ì‘ì—… ì •ë³´:
- ì œëª©: {task.get('title', '')}
- ì…ë ¥: {', '.join(task.get('inputs', []))}
- ì˜ˆìƒ ì¶œë ¥: {', '.join(task.get('outputs', []))}

ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬:
{chr(10).join(tools_info)}

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
{{
    "selected_tool_index": 1,
    "fitness_score": 0.85,
    "reasoning": "ì´ ë„êµ¬ê°€ ê°€ì¥ ì í•©í•œ ì´ìœ ë¥¼ ì„¤ëª…"
}}

ì í•©ë„ ê¸°ì¤€:
- 0.9-1.0: ë§¤ìš° ì í•© (ê±°ì˜ ì™„ë²½í•œ ë§¤ì¹˜)
- 0.7-0.9: ì í•© (ì¢‹ì€ ë§¤ì¹˜)
- 0.5-0.7: ë³´í†µ (ì–´ëŠ ì •ë„ ì í•©)
- 0.3-0.5: ë¶€ì í•© (ì•½ê°„ì˜ ê´€ë ¨ì„±)
- 0.0-0.3: ë§¤ìš° ë¶€ì í•© (ê´€ë ¨ì„± ì—†ìŒ)"""
            
            parser = JsonOutputParser()
            chain = self.llm | parser
            
            result = await chain.ainvoke([HumanMessage(content=prompt)])
            
            # ê²°ê³¼ ê²€ì¦ ë° ì²˜ë¦¬
            selected_index = result.get("selected_tool_index", 1) - 1  # 1-based to 0-based
            fitness_score = float(result.get("fitness_score", 0.5))
            reasoning = result.get("reasoning", "No reasoning provided")
            
            # ì¸ë±ìŠ¤ ë²”ìœ„ ê²€ì¦
            if 0 <= selected_index < len(available_tools):
                selected_tool = available_tools[selected_index]
            else:
                logger.warning(f"ì˜ëª»ëœ ë„êµ¬ ì¸ë±ìŠ¤: {selected_index}, ê¸°ë³¸ ë„êµ¬ ì‚¬ìš©")
                selected_tool = available_tools[0] if available_tools else None
                fitness_score = 0.5
            
            logger.info(f"ë„êµ¬ ì„ íƒ ê²°ê³¼: {selected_tool['name']} (ì í•©ë„: {fitness_score:.2f})")
            logger.info(f"ì„ íƒ ì´ìœ : {reasoning}")
            
            return {
                "tool": selected_tool,
                "fitness_score": fitness_score,
                "reasoning": reasoning
            }
            
        except Exception as e:
            logger.error(f"ë„êµ¬ ì„ íƒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return {
                "tool": available_tools[0] if available_tools else None,
                "fitness_score": 0.5,
                "reasoning": f"ì˜¤ë¥˜ë¡œ ì¸í•œ ê¸°ë³¸ ì„ íƒ: {str(e)}"
            }

    async def _execute_with_selected_tool(self, task: Dict[str, Any], tool: Dict[str, Any], existing_results: Dict[str, Any] = None) -> Dict[str, Any]:
        """ì„ íƒëœ ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‘ì—… ì‹¤í–‰"""
        tool_name = tool.get("name", "unknown")
        tool_object = tool.get("tool_object")
        logger.info(f"ë„êµ¬ '{tool_name}'ì„ ì‚¬ìš©í•˜ì—¬ ì‘ì—… ì‹¤í–‰")
        
        try:
            # ì‹¤ì œ ë„êµ¬ ê°ì²´ê°€ ìˆëŠ” ê²½ìš° ë„êµ¬ ì‹¤í–‰
            if tool_object:
                # ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
                user_input = self.input_value.content if hasattr(self.input_value, 'content') else str(self.input_value)
                
                # ë„êµ¬ ì‹¤í–‰ì„ ìœ„í•œ ì…ë ¥ ì¤€ë¹„
                tool_input = {
                    "task": task,
                    "user_input": user_input,
                    "existing_results": existing_results
                }
                
                # ë„êµ¬ê°€ í˜¸ì¶œ ê°€ëŠ¥í•œì§€ í™•ì¸
                if callable(tool_object):
                    # evaluate_expression ë„êµ¬ì˜ ê²½ìš° íŠ¹ë³„ ì²˜ë¦¬
                    if tool_name == "evaluate_expression":
                        if asyncio.iscoroutinefunction(tool_object):
                            result = await tool_object(user_input)
                        else:
                            result = tool_object(user_input)
                    else:
                        # ë„êµ¬ê°€ í•¨ìˆ˜ì¸ ê²½ìš°
                        if asyncio.iscoroutinefunction(tool_object):
                            result = await tool_object(**tool_input)
                        else:
                            result = tool_object(**tool_input)
                elif hasattr(tool_object, 'invoke'):
                    # LangChain ë„êµ¬ì¸ ê²½ìš°
                    try:
                        # evaluate_expression ë„êµ¬ì˜ ê²½ìš° íŠ¹ë³„ ì²˜ë¦¬
                        if tool_name == "evaluate_expression":
                            if asyncio.iscoroutinefunction(tool_object.invoke):
                                result = await tool_object.ainvoke(user_input)
                            else:
                                result = tool_object.invoke(user_input)
                        else:
                            if asyncio.iscoroutinefunction(tool_object.invoke):
                                result = await tool_object.ainvoke(tool_input)
                            else:
                                result = tool_object.invoke(tool_input)
                    except TypeError as e:
                        # ë„êµ¬ê°€ tool_input í˜•ì‹ì„ ë°›ì§€ ëª»í•˜ëŠ” ê²½ìš°, user_inputë§Œ ì „ë‹¬
                        logger.warning(f"ë„êµ¬ '{tool_name}'ì´ tool_input í˜•ì‹ì„ ë°›ì§€ ëª»í•¨: {e}. user_inputë§Œ ì „ë‹¬í•©ë‹ˆë‹¤.")
                        if asyncio.iscoroutinefunction(tool_object.invoke):
                            result = await tool_object.ainvoke(user_input)
                        else:
                            result = tool_object.invoke(user_input)
                elif hasattr(tool_object, 'run'):
                    # ë‹¤ë¥¸ í˜•íƒœì˜ ë„êµ¬ì¸ ê²½ìš°
                    if asyncio.iscoroutinefunction(tool_object.run):
                        result = await tool_object.run(user_input)
                    else:
                        result = tool_object.run(user_input)
                else:
                    logger.warning(f"ë„êµ¬ '{tool_name}'ì˜ ì‹¤í–‰ ë°©ë²•ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    return await self._fallback_to_direct_processing(task, user_input)
                
                # ë„êµ¬ ì‹¤í–‰ ê²°ê³¼ë¥¼ í‘œì¤€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                return {
                    "task_id": task["id"],
                    "execution_method": "tool_execution",
                    "tool_used": tool_name,
                    "status": "completed",
                    "result": str(result) if result is not None else "No result",
                    "raw_result": result
                }
            else:
                logger.warning(f"ë„êµ¬ '{tool_name}'ì˜ ê°ì²´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return await self._fallback_to_direct_processing(task, self.input_value.content if hasattr(self.input_value, 'content') else str(self.input_value))
                
        except Exception as e:
            logger.error(f"ë„êµ¬ '{tool_name}' ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return {
                "task_id": task["id"],
                "execution_method": "tool_execution_error",
                "tool_used": tool_name,
                "status": "error",
                "error": str(e),
                "result": f"ë„êµ¬ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
            }

    async def _fallback_to_direct_processing(self, task: Dict[str, Any], user_input: str) -> Dict[str, Any]:
        """ë„êµ¬ ì‹¤í–‰ ì‹¤íŒ¨ ì‹œ ì§ì ‘ ì²˜ë¦¬ë¡œ í´ë°±"""
        task_title = task.get("title", "")
        logger.info(f"ì§ì ‘ ì²˜ë¦¬ë¡œ í´ë°±: {task_title}")
        
        # ì‘ì—… ìœ í˜•ì— ë”°ë¥¸ ì§ì ‘ ì²˜ë¦¬
        if "ë¶„ì„" in task_title or "í…ìŠ¤íŠ¸" in task_title:
            return await self.handle_text_analysis_task(task, user_input)
        elif "ì§ˆë¬¸" in task_title or "ë‹µë³€" in task_title or "QA" in task_title:
            return await self.handle_qa_task(task, user_input)
        elif "ìš”ì•½" in task_title or "ì •ë¦¬" in task_title:
            return await self.handle_summary_task(task, user_input)
        elif "ìƒì„±" in task_title or "ì‘ì„±" in task_title:
            return await self.handle_generation_task(task, user_input)
        else:
            return await self.handle_general_task(task, user_input)

    async def _handle_math_calculation_task(self, task: Dict[str, Any], existing_results: Dict[str, Any] = None) -> Dict[str, Any]:
        """ìˆ˜í•™ ê³„ì‚° ì‘ì—… ì²˜ë¦¬"""
        task_id = task["id"]
        task_title = task["title"]
        
        if existing_results:
            # ì´ì „ ì‘ì—… ê²°ê³¼ì—ì„œ í•„ìš”í•œ ë°ì´í„° ì¶”ì¶œ
            word_count = None
            token_count = None
            
            for prev_task_id, prev_result in existing_results.items():
                if prev_result.get("status") == "completed":
                    result_data = prev_result.get("result", {})
                    if isinstance(result_data, dict) and "agent_response" in result_data:
                        # A2A ì—ì´ì „íŠ¸ ì‘ë‹µì—ì„œ ë°ì´í„° ì¶”ì¶œ
                        agent_response = result_data["agent_response"]
                        if isinstance(agent_response, dict) and "result" in agent_response:
                            response_text = agent_response["result"]
                            if isinstance(response_text, dict) and "parts" in response_text:
                                for part in response_text["parts"]:
                                    if part.get("kind") == "text":
                                        text_content = part.get("text", "")
                                        # í…ìŠ¤íŠ¸ì—ì„œ ë‹¨ì–´ ìˆ˜ì™€ í† í° ìˆ˜ ì¶”ì¶œ
                                        import re
                                        word_match = re.search(r'ë‹¨ì–´.*?(\d+)', text_content)
                                        token_match = re.search(r'í† í°.*?(\d+)', text_content)
                                        
                                        if word_match:
                                            word_count = int(word_match.group(1))
                                        if token_match:
                                            token_count = int(token_match.group(1))
            
            if word_count is not None and token_count is not None:
                import math
                product = word_count * token_count
                natural_log = math.log(product)
                
                result = {
                    "task_id": task_id,
                    "execution_method": "tool_math_calculation",
                    "tool_used": "math_calculator",
                    "inputs": {
                        "word_count": word_count,
                        "token_count": token_count
                    },
                    "calculation": {
                        "product": product,
                        "natural_log": natural_log
                    },
                    "result": f"ë‹¨ì–´ ìˆ˜({word_count}) Ã— í† í° ìˆ˜({token_count}) = {product}, ln({product}) = {natural_log:.6f}"
                }
                logger.info(f"ìˆ˜í•™ ê³„ì‚° ì™„ë£Œ: {result['result']}")
                return result
            else:
                logger.warning(f"ì´ì „ ì‘ì—… ê²°ê³¼ì—ì„œ ë‹¨ì–´ ìˆ˜({word_count}) ë˜ëŠ” í† í° ìˆ˜({token_count})ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        else:
            logger.warning("ìˆ˜í•™ ê³„ì‚°ì„ ìœ„í•œ ì´ì „ ì‘ì—… ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        # ìˆ˜í•™ ê³„ì‚°ì´ ë¶ˆê°€ëŠ¥í•œ ê²½ìš° ê¸°ë³¸ ì²˜ë¦¬
        return await self.handle_general_task(task, self.input_value.content if hasattr(self.input_value, 'content') else str(self.input_value))

    async def execute_task_directly(self, task: Dict[str, Any], existing_results: Dict[str, Any] = None) -> Dict[str, Any]:
        """ì§ì ‘ ì‹¤í–‰: LLMì„ ì´ìš©í•œ ë„êµ¬ ì„ íƒ ë° ì‹¤í–‰"""
        
        task_id = task["id"]
        task_title = task["title"]
        task_inputs = task.get("inputs", [])
        task_outputs = task.get("outputs", [])
        
        logger.info(f"Task({task_title}) directly execute with tool selection.\r\n inputs: {task_inputs}\r\n expected: {task_outputs}")
        
        try:
            # 1. ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
            available_tools = await self._get_available_tools()
            logger.info(f"ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬: {len(available_tools)}ê°œ")
            
            # ë„êµ¬ê°€ ì—†ëŠ” ê²½ìš° ì§ì ‘ ì²˜ë¦¬
            if not available_tools:
                logger.info("ì—°ê²°ëœ ë„êµ¬ê°€ ì—†ì–´ ì§ì ‘ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
                user_input = self.input_value.content if hasattr(self.input_value, 'content') else str(self.input_value)
                result = await self._fallback_to_direct_processing(task, user_input)
                
                # ê²°ê³¼ì— ì§ì ‘ ì²˜ë¦¬ ì •ë³´ ì¶”ê°€
                if isinstance(result, dict):
                    result["tool_selection"] = {
                        "tool_used": "direct_processing",
                        "fitness_score": 0.0,
                        "reasoning": "ì—°ê²°ëœ ë„êµ¬ê°€ ì—†ì–´ ì§ì ‘ ì²˜ë¦¬"
                    }
                
                return result
            
            # 2. LLMì„ ì‚¬ìš©í•˜ì—¬ ê°€ì¥ ì í•©í•œ ë„êµ¬ ì„ íƒ
            tool_selection = await self._select_best_tool(task, available_tools)
            selected_tool = tool_selection["tool"]
            fitness_score = tool_selection["fitness_score"]
            reasoning = tool_selection["reasoning"]
            
            # 3. ì í•©ë„ê°€ 0.7 ì´ìƒì¸ ê²½ìš° ë„êµ¬ ì‚¬ìš©, ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ ì§ì ‘ ì²˜ë¦¬
            if fitness_score >= 0.7 and selected_tool:
                logger.info(f"ë„êµ¬ ì‚¬ìš© ê²°ì •: {selected_tool['name']} (ì í•©ë„: {fitness_score:.2f})")
                result = await self._execute_with_selected_tool(task, selected_tool, existing_results)
                
                # ê²°ê³¼ì— ë„êµ¬ ì •ë³´ ì¶”ê°€
                if isinstance(result, dict):
                    result["tool_selection"] = {
                        "tool_used": selected_tool["name"],
                        "fitness_score": fitness_score,
                        "reasoning": reasoning
                    }
                
                return result
            else:
                logger.info(f"ë„êµ¬ ì‚¬ìš© ê±°ë¶€: ì í•©ë„ {fitness_score:.2f} < 0.7, ì§ì ‘ ì²˜ë¦¬")
                # ì§ì ‘ ì²˜ë¦¬
                user_input = self.input_value.content if hasattr(self.input_value, 'content') else str(self.input_value)
                result = await self._fallback_to_direct_processing(task, user_input)
                
                # ê²°ê³¼ì— ì§ì ‘ ì²˜ë¦¬ ì •ë³´ ì¶”ê°€
                if isinstance(result, dict):
                    result["tool_selection"] = {
                        "tool_used": "direct_processing",
                        "fitness_score": fitness_score,
                        "reasoning": f"ì í•©ë„ ë¶€ì¡±ìœ¼ë¡œ ì§ì ‘ ì²˜ë¦¬: {reasoning}"
                    }
                
                return result
                
        except Exception as e:
            logger.error(f"ì‘ì—… ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return {
                "task_id": task_id,
                "execution_method": "direct_with_error",
                "status": "error",
                "error": str(e),
                "result": f"ì‘ì—… '{task_title}' ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
            }

    async def handle_text_analysis_task(self, task: Dict[str, Any], user_input: str) -> Dict[str, Any]:
        """í…ìŠ¤íŠ¸ ë¶„ì„ ì‘ì—… ì²˜ë¦¬"""
        logger.info("í…ìŠ¤íŠ¸ ë¶„ì„ ì‘ì—… ì‹¤í–‰")
        
        # ê¸°ë³¸ í…ìŠ¤íŠ¸ ë¶„ì„ ìˆ˜í–‰
        word_count = len(user_input.split())
        char_count = len(user_input)
        char_count_no_spaces = len(user_input.replace(' ', ''))
        
        # ë¬¸ì¥ ìˆ˜ ê³„ì‚°
        import re
        sentences = re.split(r'[.!?]+', user_input)
        sentence_count = len([s for s in sentences if s.strip()])
        
        # ë‹¨ë½ ìˆ˜ ê³„ì‚°
        paragraphs = user_input.split('\n\n')
        paragraph_count = len([p for p in paragraphs if p.strip()])
        
        analysis_result = {
            "í…ìŠ¤íŠ¸": user_input[:100] + "..." if len(user_input) > 100 else user_input,
            "ë‹¨ì–´_ìˆ˜": word_count,
            "ë¬¸ì_ìˆ˜_ê³µë°±í¬í•¨": char_count,
            "ë¬¸ì_ìˆ˜_ê³µë°±ì œì™¸": char_count_no_spaces,
            "ë¬¸ì¥_ìˆ˜": sentence_count,
            "ë‹¨ë½_ìˆ˜": paragraph_count
        }
        
        result_text = f"""ğŸ“Š í…ìŠ¤íŠ¸ ë¶„ì„ ê²°ê³¼:
        
ğŸ”¤ ë‹¨ì–´ ìˆ˜: {word_count}ê°œ
ğŸ“ ë¬¸ì ìˆ˜ (ê³µë°± í¬í•¨): {char_count}ê°œ
âœï¸ ë¬¸ì ìˆ˜ (ê³µë°± ì œì™¸): {char_count_no_spaces}ê°œ
ğŸ“„ ë¬¸ì¥ ìˆ˜: {sentence_count}ê°œ
ğŸ“‘ ë‹¨ë½ ìˆ˜: {paragraph_count}ê°œ

ğŸ“– ë¶„ì„ëœ í…ìŠ¤íŠ¸: "{user_input[:100]}{'...' if len(user_input) > 100 else ''}"
        """
        
        return {
            "task_id": task["id"],
            "execution_method": "text_analysis",
            "status": "completed",
            "analysis": analysis_result,
            "result": result_text
        }

    async def handle_qa_task(self, task: Dict[str, Any], user_input: str) -> Dict[str, Any]:
        """ì§ˆì˜ì‘ë‹µ ì‘ì—… ì²˜ë¦¬"""
        logger.info("ì§ˆì˜ì‘ë‹µ ì‘ì—… ì‹¤í–‰")
        
        # LLMì´ ìˆëŠ” ê²½ìš° LLMì„ ì‚¬ìš©
        if hasattr(self, 'llm') and self.llm is not None:
            try:
                from langchain_core.messages import HumanMessage
                
                prompt = f"""ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•´ ì •í™•í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”:

ì§ˆë¬¸: {user_input}

ë‹µë³€ì€ ëª…í™•í•˜ê³  ì´í•´í•˜ê¸° ì‰½ê²Œ ì‘ì„±í•´ì£¼ì„¸ìš”."""
                
                response = await self.llm.ainvoke([HumanMessage(content=prompt)])
                answer = response.content if hasattr(response, 'content') else str(response)
                
                return {
                    "task_id": task["id"],
                    "execution_method": "llm_qa",
                    "status": "completed",
                    "question": user_input,
                    "answer": answer,
                    "result": f"â“ ì§ˆë¬¸: {user_input}\n\nğŸ’¡ ë‹µë³€: {answer}"
                }
            except Exception as e:
                logger.error(f"LLM ì§ˆì˜ì‘ë‹µ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        
        # LLMì´ ì—†ê±°ë‚˜ ì˜¤ë¥˜ê°€ ë°œìƒí•œ ê²½ìš° ê¸°ë³¸ ì‘ë‹µ
        default_answer = f"'{user_input}'ì— ëŒ€í•œ ì§ˆë¬¸ì„ ë°›ì•˜ìŠµë‹ˆë‹¤. ë” êµ¬ì²´ì ì¸ ë‹µë³€ì„ ìœ„í•´ì„œëŠ” LLM ëª¨ë¸ì´ í•„ìš”í•©ë‹ˆë‹¤."
        
        return {
            "task_id": task["id"],
            "execution_method": "default_qa",
            "status": "completed",
            "question": user_input,
            "answer": default_answer,
            "result": f"â“ ì§ˆë¬¸: {user_input}\n\nğŸ’¡ ë‹µë³€: {default_answer}"
        }

    async def handle_summary_task(self, task: Dict[str, Any], user_input: str) -> Dict[str, Any]:
        """ìš”ì•½ ì‘ì—… ì²˜ë¦¬"""
        logger.info("ìš”ì•½ ì‘ì—… ì‹¤í–‰")
        
        # LLMì´ ìˆëŠ” ê²½ìš° LLMì„ ì‚¬ìš©
        if hasattr(self, 'llm') and self.llm is not None:
            try:
                from langchain_core.messages import HumanMessage
                
                prompt = f"""ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ í•µì‹¬ ë‚´ìš©ì„ í¬í•¨í•˜ì—¬ ê°„ê²°í•˜ê²Œ ìš”ì•½í•´ì£¼ì„¸ìš”:

ì›ë³¸ í…ìŠ¤íŠ¸:
{user_input}

ìš”ì•½ì€ 3-5ê°œì˜ ì£¼ìš” í¬ì¸íŠ¸ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”."""
                
                response = await self.llm.ainvoke([HumanMessage(content=prompt)])
                summary = response.content if hasattr(response, 'content') else str(response)
                
                return {
                    "task_id": task["id"],
                    "execution_method": "llm_summary",
                    "status": "completed",
                    "original_text": user_input,
                    "summary": summary,
                    "result": f"ğŸ“„ ì›ë³¸ ê¸¸ì´: {len(user_input)}ì\n\nğŸ“ ìš”ì•½:\n{summary}"
                }
            except Exception as e:
                logger.error(f"LLM ìš”ì•½ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        
        # LLMì´ ì—†ê±°ë‚˜ ì˜¤ë¥˜ê°€ ë°œìƒí•œ ê²½ìš° ê¸°ë³¸ ìš”ì•½
        words = user_input.split()
        if len(words) > 50:
            # ì²« 30ë‹¨ì–´ì™€ ë§ˆì§€ë§‰ 20ë‹¨ì–´ë¡œ ê¸°ë³¸ ìš”ì•½
            summary = " ".join(words[:30]) + " ... " + " ".join(words[-20:])
        else:
            summary = user_input
        
        return {
            "task_id": task["id"],
            "execution_method": "basic_summary",
            "status": "completed",
            "original_text": user_input,
            "summary": summary,
            "result": f"ğŸ“„ ì›ë³¸ ê¸¸ì´: {len(user_input)}ì\n\nğŸ“ ê¸°ë³¸ ìš”ì•½:\n{summary}"
        }

    async def handle_generation_task(self, task: Dict[str, Any], user_input: str) -> Dict[str, Any]:
        """ìƒì„± ì‘ì—… ì²˜ë¦¬"""
        logger.info("ìƒì„± ì‘ì—… ì‹¤í–‰")
        
        # LLMì´ ìˆëŠ” ê²½ìš° LLMì„ ì‚¬ìš©
        if hasattr(self, 'llm') and self.llm is not None:
            try:
                from langchain_core.messages import HumanMessage
                
                prompt = f"""ë‹¤ìŒ ìš”ì²­ì— ë”°ë¼ ì°½ì˜ì ì´ê³  ìœ ìš©í•œ ë‚´ìš©ì„ ìƒì„±í•´ì£¼ì„¸ìš”:

ìš”ì²­: {user_input}

ì ì ˆí•œ í˜•ì‹ê³¼ êµ¬ì¡°ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”."""
                
                response = await self.llm.ainvoke([HumanMessage(content=prompt)])
                generated_content = response.content if hasattr(response, 'content') else str(response)
                
                return {
                    "task_id": task["id"],
                    "execution_method": "llm_generation",
                    "status": "completed",
                    "request": user_input,
                    "generated_content": generated_content,
                    "result": f"ğŸ¯ ìš”ì²­: {user_input}\n\nâœ¨ ìƒì„±ëœ ë‚´ìš©:\n{generated_content}"
                }
            except Exception as e:
                logger.error(f"LLM ìƒì„± ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        
        # LLMì´ ì—†ê±°ë‚˜ ì˜¤ë¥˜ê°€ ë°œìƒí•œ ê²½ìš° ê¸°ë³¸ ì‘ë‹µ
        default_content = f"'{user_input}' ìš”ì²­ì„ ë°›ì•˜ìŠµë‹ˆë‹¤. ë” ì •êµí•œ ìƒì„±ì„ ìœ„í•´ì„œëŠ” LLM ëª¨ë¸ì´ í•„ìš”í•©ë‹ˆë‹¤."
        
        return {
            "task_id": task["id"],
            "execution_method": "default_generation",
            "status": "completed",
            "request": user_input,
            "generated_content": default_content,
            "result": f"ğŸ¯ ìš”ì²­: {user_input}\n\nâœ¨ ê¸°ë³¸ ì‘ë‹µ:\n{default_content}"
        }

    async def handle_general_task(self, task: Dict[str, Any], user_input: str) -> Dict[str, Any]:
        """ì¼ë°˜ì ì¸ ì‘ì—… ì²˜ë¦¬"""
        logger.info("ì¼ë°˜ ì‘ì—… ì‹¤í–‰")
        
        # ê¸°ë³¸ì ì¸ ì‘ì—… ì²˜ë¦¬
        task_description = task.get("title", "ì¼ë°˜ ì‘ì—…")
        
        # LLMì´ ìˆëŠ” ê²½ìš° LLMì„ ì‚¬ìš©
        if hasattr(self, 'llm') and self.llm is not None:
            try:
                from langchain_core.messages import HumanMessage
                
                prompt = f"""ë‹¤ìŒ ì‘ì—…ì„ ìˆ˜í–‰í•´ì£¼ì„¸ìš”:

ì‘ì—…: {task_description}
ì‚¬ìš©ì ì…ë ¥: {user_input}

ì ì ˆí•œ ë°©ì‹ìœ¼ë¡œ ì‘ì—…ì„ ì™„ë£Œí•˜ê³  ê²°ê³¼ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”."""
                
                response = await self.llm.ainvoke([HumanMessage(content=prompt)])
                result_content = response.content if hasattr(response, 'content') else str(response)
                
                return {
                    "task_id": task["id"],
                    "execution_method": "llm_general",
                    "status": "completed",
                    "task_description": task_description,
                    "user_input": user_input,
                    "result": f"ğŸ”§ ì‘ì—…: {task_description}\nğŸ“ ì…ë ¥: {user_input}\n\nâœ… ê²°ê³¼:\n{result_content}"
                }
            except Exception as e:
                logger.error(f"LLM ì¼ë°˜ ì‘ì—… ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        
        # LLMì´ ì—†ê±°ë‚˜ ì˜¤ë¥˜ê°€ ë°œìƒí•œ ê²½ìš° ê¸°ë³¸ ì²˜ë¦¬
        result_content = f"'{task_description}' ì‘ì—…ì„ '{user_input}' ì…ë ¥ìœ¼ë¡œ ì²˜ë¦¬í–ˆìŠµë‹ˆë‹¤. ë” ì •êµí•œ ì²˜ë¦¬ë¥¼ ìœ„í•´ì„œëŠ” LLM ëª¨ë¸ì´ë‚˜ ì „ìš© ë„êµ¬ê°€ í•„ìš”í•©ë‹ˆë‹¤."
        
        return {
            "task_id": task["id"],
            "execution_method": "basic_general",
            "status": "completed",
            "task_description": task_description,
            "user_input": user_input,
            "result": f"ğŸ”§ ì‘ì—…: {task_description}\nğŸ“ ì…ë ¥: {user_input}\n\nâœ… ê¸°ë³¸ ì²˜ë¦¬ ê²°ê³¼:\n{result_content}"
                }

    def find_parts(self, obj: Dict[str, Any]) -> list[Dict[str, Any]]:
        """
        JSON(dict/list) êµ¬ì¡° ë‚´ë¶€ë¥¼ ì¬ê·€ì ìœ¼ë¡œ íƒìƒ‰í•˜ì—¬ 
        'parts' í•„ë“œë¡œ ì§€ì •ëœ ë‚´ìš©ì„ ëª¨ë‘ ì°¾ì•„ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜
        """
        results = []

        if isinstance(obj, dict):
            # í˜„ì¬ dictì— 'parts' í‚¤ê°€ ìˆìœ¼ë©´ ì¶”ê°€
            if "parts" in obj:
                parts_content = obj["parts"]
                # partsê°€ ì´ë¯¸ ë¦¬ìŠ¤íŠ¸ì´ë©´ ê·¸ëŒ€ë¡œ ì¶”ê°€, ì•„ë‹ˆë©´ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
                if isinstance(parts_content, list):
                    results.extend(parts_content)
                else:
                    results.append(parts_content)

            # ì¬ê·€ì ìœ¼ë¡œ ëª¨ë“  ê°’ë“¤ì„ íƒìƒ‰
            for value in obj.values():
                results.extend(self.find_parts(value))

        elif isinstance(obj, list):
            # ë¦¬ìŠ¤íŠ¸ì˜ ê° í•­ëª©ì„ ì¬ê·€ì ìœ¼ë¡œ íƒìƒ‰
            for item in obj:
                results.extend(self.find_parts(item))

        return results

    def _find_value_recursive(self, obj: Any, target_key: str, default_value: Any = None) -> Any:
        """
        ì¤‘ì²©ëœ dict/list êµ¬ì¡°ì—ì„œ íŠ¹ì • í‚¤ì˜ ê°’ì„ ì¬ê·€ì ìœ¼ë¡œ ì°¾ëŠ” í•¨ìˆ˜
        
        Args:
            obj: íƒìƒ‰í•  ê°ì²´ (dict, list, ë˜ëŠ” ê¸°íƒ€)
            target_key: ì°¾ì„ í‚¤ ì´ë¦„
            default_value: ì°¾ì§€ ëª»í–ˆì„ ë•Œ ë°˜í™˜í•  ê¸°ë³¸ê°’
            
        Returns:
            ì°¾ì€ ê°’ ë˜ëŠ” ê¸°ë³¸ê°’
        """
        if isinstance(obj, dict):
            # í˜„ì¬ dictì— íƒ€ê²Ÿ í‚¤ê°€ ìˆìœ¼ë©´ ë°˜í™˜
            if target_key in obj:
                return obj[target_key]
            
            # ëª¨ë“  ê°’ë“¤ì„ ì¬ê·€ì ìœ¼ë¡œ íƒìƒ‰
            for value in obj.values():
                result = self._find_value_recursive(value, target_key, default_value)
                if result is not default_value:
                    return result
                    
        elif isinstance(obj, list):
            # ë¦¬ìŠ¤íŠ¸ì˜ ê° ìš”ì†Œë¥¼ ìˆœì„œëŒ€ë¡œ íƒìƒ‰
            for item in obj:
                result = self._find_value_recursive(item, target_key, default_value)
                if result is not default_value:
                    return result
        
        return default_value

    def _extract_session_id(self, parts: list) -> str:
        """partsì—ì„œ session_idë¥¼ ì¬ê·€ì ìœ¼ë¡œ ì¶”ì¶œ"""
        if not parts:
            return "unknown"
        
        result = self._find_value_recursive(parts, "session_id", "unknown")
        return result if result != "unknown" else "unknown"

    def _extract_input_text(self, parts: list) -> str:
        """partsì—ì„œ input_valueë¥¼ ì¬ê·€ì ìœ¼ë¡œ ì¶”ì¶œ"""
        if not parts:
            return "unknown"
        
        result = self._find_value_recursive(parts, "input_value", "unknown")
        return result if result != "unknown" else "unknown"

    def _extract_analysis_text(self, parts: list) -> str:
        """partsì—ì„œ ìµœì¢… í…ìŠ¤íŠ¸ë¥¼ ì¬ê·€ì ìœ¼ë¡œ ì¶”ì¶œ"""
        if not parts:
            return None
        
        # message.data.text ê²½ë¡œë¥¼ ë”°ë¼ ì°¾ê¸°
        result = self._find_value_recursive(parts, "data", None)
        
        text = ""
        if  result:
            text=result["text"]

        return text

    def extract_text_from_agent_response(self, agent_response: Dict[str, Any]) -> Dict[str, Any]:
        """A2A ì—ì´ì „íŠ¸ ì‘ë‹µì—ì„œ ì‹¤ì œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ëŠ” í—¬í¼ í•¨ìˆ˜"""
        try:
            # agent_responseê°€ ì´ë¯¸ dictì¸ ê²½ìš°
            if isinstance(agent_response, dict):
                obj = agent_response
            # agent_responseê°€ ë¬¸ìì—´ì¸ ê²½ìš° JSON íŒŒì‹±
            elif isinstance(agent_response, str):
                if agent_response.strip().startswith('{'):
                    obj = json.loads(agent_response)
                else:
                    logger.warning("Agent response is not a valid JSON string")
                    return None
            else:
                logger.warning(f"Unexpected agent_response type: {type(agent_response)}")
                return None
            
            # JSON êµ¬ì¡°ì—ì„œ ì¤‘ì²©ëœ JSON ë¬¸ìì—´ íŒŒì‹±
            obj = self._parse_nested_json_strings(obj)
            
            # parts í•„ë“œë“¤ ì°¾ê¸°
            parts = self.find_parts(obj)
            
            if len(parts) == 0:
                logger.warning("No message fields found in agent response")
                return None

            #logger.info(f"agent_response parts: {parts[0]}") 
            
            # ì•ˆì „í•œ ë°ì´í„° ì¶”ì¶œ
            try:
                session_id = self._extract_session_id(parts)
                input_text = self._extract_input_text(parts)
                analysis_text = self._extract_analysis_text(parts)
                
                logger.info(f"session_id: {session_id}")
                logger.info(f"input_text: {input_text}")
                logger.info(f"analysis_text: {analysis_text}")
                
            except Exception as e:
                logger.error(f"Error extracting data from parts: {e}")
                logger.error(f"Parts structure: {parts}")
                return None 
            
            if not analysis_text:
                logger.warning("No meaningful text content found in messages")
                return None
               
            return {
                "session_id": session_id,
                "input_text": input_text,
                "analysis_text": analysis_text
            }
        except Exception as e:
            logger.error(f"Error extracting text from agent response: {e}")
            return None

    def _parse_nested_json_strings(self, obj):
        """ì¤‘ì²©ëœ JSON ë¬¸ìì—´ì„ ì¬ê·€ì ìœ¼ë¡œ íŒŒì‹±"""
        if isinstance(obj, dict):
            result = {}
            for key, value in obj.items():
                if isinstance(value, str) and value.strip().startswith('{'):
                    try:
                        result[key] = self._parse_nested_json_strings(json.loads(value))
                    except json.JSONDecodeError:
                        result[key] = value
                else:
                    result[key] = self._parse_nested_json_strings(value)
            return result
        elif isinstance(obj, list):
            return [self._parse_nested_json_strings(item) for item in obj]
        else:
            return obj

    def _extract_best_text_from_messages(self, msgs):
        """ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ì—ì„œ ê°€ì¥ ìœ ìš©í•œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œ"""
        for msg in msgs:
            if not isinstance(msg, dict):
                continue
                
            # ìš°ì„ ìˆœìœ„ 1: data.text (ê°€ì¥ êµ¬ì¡°í™”ëœ í…ìŠ¤íŠ¸)
            if "data" in msg and isinstance(msg["data"], dict) and "text" in msg["data"]:
                text = msg["data"]["text"]
                if text and len(text.strip()) > 10:  # ì˜ë¯¸ìˆëŠ” í…ìŠ¤íŠ¸ì¸ì§€ í™•ì¸
                    return text
            
            # ìš°ì„ ìˆœìœ„ 2: text (ì§ì ‘ í…ìŠ¤íŠ¸)
            if "text" in msg:
                text = msg["text"]
                if text and len(text.strip()) > 10:
                    return text
            
            # ìš°ì„ ìˆœìœ„ 3: content (ë˜í•‘ëœ í…ìŠ¤íŠ¸)
            if "content" in msg:
                content = msg["content"]
                if content and len(str(content).strip()) > 10:
                    return str(content)
        
        return None

    async def delegate_task_to_agent(self, task: Dict[str, Any], agent_id: str, available_agents: List[Dict[str, Any]]) -> Dict[str, Any]:
        """íŠ¹ì • ì—ì´ì „íŠ¸ì—ê²Œ ì‘ì—…ì„ ìœ„ì„"""
        
        logger.info(f"Delegating task {task['id']} to agent {agent_id}")
        
        target_agent = None
        
        for agent in available_agents:
            if agent.get("id") == agent_id:
                target_agent = agent
                break
        
        if not target_agent:
            raise ValueError(f"Pre-assigned agent '{agent_id}' not found in available agents")
        
        # ì—ì´ì „íŠ¸ URL í™•ì¸ ë° ë³€í™˜
        agent_url = target_agent.get("url") or target_agent.get("endpoint")
        if not agent_url:
            agent_name = target_agent.get("name", target_agent.get("id", "Unknown"))
            msg = f"ì„ íƒëœ ì—ì´ì „íŠ¸ '{agent_name}' (ID: {target_agent.get('id', 'Unknown')})ì— URLì´ ì§€ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì—ì´ì „íŠ¸ ì„¤ì •ì„ í™•ì¸í•´ì£¼ì„¸ìš”."
            raise ValueError(msg)
        
        # 0.0.0.0ì„ 127.0.0.1ë¡œ ë³€í™˜ (í´ë¼ì´ì–¸íŠ¸ í˜¸ì¶œìš©)
        if "0.0.0.0" in agent_url:
            agent_url = agent_url.replace("0.0.0.0", "127.0.0.1")
        
        # A2A í”„ë¡œí† ì½œì— ë§ëŠ” ì‹¤ì œ API í˜¸ì¶œ
        try:
            logger.info(f"Calling A2A API: {agent_url}")
            
            # A2A JSON-RPC 2.0 í”„ë¡œí† ì½œ í˜ì´ë¡œë“œ êµ¬ì„±
            a2a_rpc_payload = {
                "jsonrpc": "2.0",
                "id": task["id"],
                "method": "invoke",
                "params": {
                    "message": {
                        "message_id": task["id"],
                        "role": "user",
                        "parts": [
                            {
                                "kind": "text",
                                "text": f"Task: {task['title']}\nDescription: {task.get('description', '')}\nInputs: {', '.join(task.get('inputs', []))}\nExpected Outputs: {', '.join(task.get('outputs', []))}"
                            }
                        ]
                    },
                    "stream": False  # ìŠ¤íŠ¸ë¦¬ë° ë¹„í™œì„±í™”
                }
            }
            
            # HTTP í—¤ë” ì„¤ì •
            headers = {
                "Content-Type": "application/json",
                "Accept": "application/json",
                "User-Agent": "A2A-Agent-Component/1.0"
            }
            # A2A í‘œì¤€ API í˜¸ì¶œ
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    agent_url,
                    json=a2a_rpc_payload,
                    headers=headers,
                    timeout=aiohttp.ClientTimeout(total=30)
                ) as response:
                                       
                    if response.status == 200:
                        try:
                            logger.info(f"Task {task['id']} successfully delegated to agent {agent_id}")
                            result_data = await response.json()
                            extracted_data = self.extract_text_from_agent_response(result_data)
                            
                            # extract_text_from_agent_responseê°€ Noneì„ ë°˜í™˜í•œ ê²½ìš° ì›ë³¸ ë°ì´í„° ì‚¬ìš©
                            if extracted_data is None:
                                raise Exception("Failed to extract text from agent response")
                                
                            return {
                                    "task_id": task["id"],
                                    "agent_id": agent_id,
                                    "status": "completed",
                                    "timestamp": "2024-01-01T00:00:00Z",
                                    "session_id": extracted_data["session_id"],
                                    "input_text": extracted_data["input_text"],
                                    "message_text": extracted_data["analysis_text"]
                                }

                        except Exception as json_error:
                            logger.error(f"Error parsing JSON response: {json_error}")
                            response_text = await response.text()
                            logger.error(f"Raw response text: {response_text}")
                            return {
                                "task_id": task["id"],
                                "agent_id": agent_id,
                                "status": "failed",
                                "error": f"JSON parsing error: {json_error}",
                                "timestamp": "2024-01-01T00:00:00Z"
                            }
                    else:
                        raise Exception("Failed to extract text from agent response")
                        
        except asyncio.TimeoutError:
            raise ValueError("Timeout while delegating task {task['id']} to agent {agent_id}")
        except Exception as e:
            raise ValueError(f"Error delegating task {task['id']} to agent {agent_id}: {e}")


    async def critic_phase(self, task_id: str, execution_results: Dict[str, Any], current_plan: Dict[str, Any]) -> Dict[str, Any]:
        """ë¹„í‰ ë‹¨ê³„: ì‹¤í–‰ ê²°ê³¼ë¥¼ ë¶„ì„í•˜ê³  ì™„ë£Œ ì—¬ë¶€ë¥¼ íŒë‹¨"""
        
        logger.info("Critic Phase Start...")
        
        # Critic Prompt í•„ìˆ˜ ì—°ê²° í™•ì¸
        if not hasattr(self, 'critic_prompt') or not self.critic_prompt:
            msg = "Critic Promptê°€ ì—°ê²°ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. Prompt Template ì»´í¬ë„ŒíŠ¸ë¥¼ ì—°ê²°í•´ì£¼ì„¸ìš”."
            logger.error(msg)
            raise ValueError(msg)
        
        return await self._llm_based_critic(task_id, execution_results, current_plan)
    
    async def _llm_based_critic(self, task_id: str, execution_results: Dict[str, Any], current_plan: Dict[str, Any]) -> Dict[str, Any]:
        """LLM ê¸°ë°˜ ë¹„í‰ ë¶„ì„: ì‹¤í–‰ ê²°ê³¼ ë¶„ì„ ë° ì™„ë£Œ ì—¬ë¶€ íŒë‹¨"""
        
        # ì™¸ë¶€ Prompt Component ì‚¬ìš©
        critic_prompt = create_prompt_from_component(
            self.critic_prompt,
            default_system_message="ë‹¹ì‹ ì€ ì‘ì—… ì‹¤í–‰ ê²°ê³¼ë¥¼ ë¶„ì„í•˜ê³  ì™„ë£Œ ì—¬ë¶€ë¥¼ íŒë‹¨í•˜ëŠ” ì „ë¬¸ ë¹„í‰ê°€ì…ë‹ˆë‹¤."
        )
        
        if critic_prompt is None:
            msg = "Critic Prompt ë³€í™˜ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì˜¬ë°”ë¥¸ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì„ ì—°ê²°í•´ì£¼ì„¸ìš”."
            logger.error(msg)
            raise ValueError(msg)
        
        try:
            # ì‹¤í–‰ ê²°ê³¼ ìš”ì•½ ìƒì„±
            total_tasks = len(current_plan.get("work_breakdown", []))
            completed_tasks = sum(1 for result in execution_results.values() if result.get("status") == "completed")
            failed_tasks = sum(1 for result in execution_results.values() if result.get("status") == "failed")
            
            # JSON ì§ë ¬í™” ì•ˆì „ ì²˜ë¦¬
            def safe_json_dumps(obj):
                try:
                    return json.dumps(obj, ensure_ascii=False, indent=2, default=str)
                except Exception as e:
                    logger.warning(f"JSON ì§ë ¬í™” ì‹¤íŒ¨, ë¬¸ìì—´ ë³€í™˜ ì‚¬ìš©: {e}")
                    return str(obj)
            
            chain = critic_prompt | self.llm | JsonOutputParser()
            result = await chain.ainvoke({
                "execution_results": safe_json_dumps(execution_results),
                "current_plan": safe_json_dumps(current_plan)
            })
            #logger.info(f"--------------------------------")
            #logger.info(f"task id: {task_id}")
            ## work_breakdownì—ì„œ idê°€ task_idì™€ ê°™ì€ ìš”ì†Œë¥¼ ì°¾ìŒ
            #task_info = next((task for task in current_plan.get('work_breakdown', []) if task.get('id') == task_id), None)
            #logger.info(f"task: {task_info}")
            #logger.info(f"--------------------------------")
            #logger.info(f"critic_result: {result}")
            #logger.info(f"--------------------------------") 

            # ê¸°ë³¸ê°’ ë³´ì¥
            if "is_complete" not in result:
                result["is_complete"] = (completed_tasks == total_tasks) and (failed_tasks == 0)
            if "analysis" not in result:
                result["analysis"] = f"ì™„ë£Œëœ ì‘ì—…: {completed_tasks}/{total_tasks}, ì‹¤íŒ¨í•œ ì‘ì—…: {failed_tasks}"
            if "recommendations" not in result:
                result["recommendations"] = ["ê³„ì† ì§„í–‰"]
                
            logger.info("LLM-based critic analysis completed")
            return result
            
        except Exception as e:
            msg=f"LLM critic analysis failed: {e}"
            logger.error(msg)
            raise ValueError(msg)   
    
    async def get_memory_data(self):
        # TODO: This is a temporary fix to avoid message duplication. We should develop a function for this.
        messages = (
            await MemoryComponent(**self.get_base_args())
            .set(session_id=self.graph.session_id, order="Ascending", n_messages=self.n_messages)
            .retrieve_messages()
        )
        return [
            message for message in messages if getattr(message, "id", None) != getattr(self.input_value, "id", None)
        ]

    def get_llm(self):
        if not isinstance(self.agent_llm, str):
            return self.agent_llm, None

        try:
            provider_info = MODEL_PROVIDERS_DICT.get(self.agent_llm)
            if not provider_info:
                msg = f"Invalid model provider: {self.agent_llm}"
                raise ValueError(msg)

            component_class = provider_info.get("component_class")
            display_name = component_class.display_name
            inputs = provider_info.get("inputs")
            prefix = provider_info.get("prefix", "")

            return self._build_llm_model(component_class, inputs, prefix), display_name

        except Exception as e:
            logger.error(f"Error building {self.agent_llm} language model: {e!s}")
            msg = f"Failed to initialize language model: {e!s}"
            raise ValueError(msg) from e

    def _build_llm_model(self, component, inputs, prefix=""):
        model_kwargs = {}
        for input_ in inputs:
            if hasattr(self, f"{prefix}{input_.name}"):
                value = getattr(self, f"{prefix}{input_.name}")
                if value is not None:  # None ê°’ ì œì™¸
                    model_kwargs[input_.name] = value
        
        try:
            if model_kwargs:
                return component.set(**model_kwargs).build_model()
            else:
                return component.build_model()
        except Exception as e:
            logger.warning(f"Component.set() ì‹¤íŒ¨, ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ ì‹œë„: {e}")
            try:
                return component.build_model()
            except Exception as e2:
                logger.error(f"Component.build_model()ë„ ì‹¤íŒ¨: {e2}")
                raise

    def set_component_params(self, component):
        provider_info = MODEL_PROVIDERS_DICT.get(self.agent_llm)
        if provider_info:
            inputs = provider_info.get("inputs")
            prefix = provider_info.get("prefix")
            model_kwargs = {input_.name: getattr(self, f"{prefix}{input_.name}") for input_ in inputs}

            return component.set(**model_kwargs)
        return component

    def delete_fields(self, build_config: dotdict, fields: dict | list[str]) -> None:
        """Delete specified fields from build_config."""
        for field in fields:
            build_config.pop(field, None)

    def update_input_types(self, build_config: dotdict) -> dotdict:
        """Update input types for all fields in build_config."""
        for key, value in build_config.items():
            if isinstance(value, dict):
                if value.get("input_types") is None:
                    build_config[key]["input_types"] = []
                # agent_llmì˜ input_typesë¥¼ LanguageModelìœ¼ë¡œ ê°•ì œ ì„¤ì •
                if key == "agent_llm":
                    build_config[key]["input_types"] = ["LanguageModel"]
            elif hasattr(value, "input_types") and value.input_types is None:
                value.input_types = []
                # agent_llmì˜ input_typesë¥¼ LanguageModelìœ¼ë¡œ ê°•ì œ ì„¤ì •
                if key == "agent_llm":
                    value.input_types = ["LanguageModel"]
        return build_config

    async def update_build_config(
        self, build_config: dotdict, field_value: str, field_name: str | None = None
    ) -> dotdict:
        # Iterate over all providers in the MODEL_PROVIDERS_DICT
        # Existing logic for updating build_config
        if field_name in ("agent_llm",):
            build_config["agent_llm"]["value"] = field_value
            provider_info = MODEL_PROVIDERS_DICT.get(field_value)
            if provider_info:
                component_class = provider_info.get("component_class")
                if component_class and hasattr(component_class, "update_build_config"):
                    # Call the component class's update_build_config method
                    build_config = await update_component_build_config(
                        component_class, build_config, field_value, "model_name"
                    )

            provider_configs: dict[str, tuple[dict, list[dict]]] = {
                provider: (
                    MODEL_PROVIDERS_DICT[provider]["fields"],
                    [
                        MODEL_PROVIDERS_DICT[other_provider]["fields"]
                        for other_provider in MODEL_PROVIDERS_DICT
                        if other_provider != provider
                    ],
                )
                for provider in MODEL_PROVIDERS_DICT
            }
            if field_value in provider_configs:
                fields_to_add, fields_to_delete = provider_configs[field_value]

                # Delete fields from other providers
                for fields in fields_to_delete:
                    self.delete_fields(build_config, fields)

                # Add provider-specific fields
                if field_value == "OpenAI" and not any(field in build_config for field in fields_to_add):
                    build_config.update(fields_to_add)
                else:
                    build_config.update(fields_to_add)
                # Reset input types for agent_llm
                build_config["agent_llm"]["input_types"] = ["LanguageModel"]
            elif field_value == "Custom":
                # Delete all provider fields
                self.delete_fields(build_config, ALL_PROVIDER_FIELDS)
                # Update with custom component
                custom_component = DropdownInput(
                    name="agent_llm",
                    display_name="Language Model",
                    options=[*sorted(MODEL_PROVIDERS), "Custom"],
                    value="Custom",
                    real_time_refresh=True,
                    input_types=["LanguageModel"],
                    options_metadata=[MODELS_METADATA[key] for key in sorted(MODELS_METADATA.keys())]
                    + [{"icon": "brain"}],
                )
                build_config.update({"agent_llm": custom_component.to_dict()})
            # Update input types for all fields
            build_config = self.update_input_types(build_config)

            # Validate required keys
            default_keys = [
                "code",
                "_type",
                "agent_llm",
                "tools",
                "input_value",
                "add_current_date_tool",
                "system_prompt",
                "agent_description",
                "a2a_max_iterations",
                "handle_parsing_errors",
                "verbose",
                "work_plan_prompt",
                "planning_prompt", 

                "critic_prompt",
            ]
            
            # ëˆ„ë½ëœ í•„ìˆ˜ í‚¤ë“¤ì„ ê¸°ë³¸ê°’ìœ¼ë¡œ ì„¤ì •
            if "tools" not in build_config:
                build_config["tools"] = {"value": "", "input_types": ["Tool"]}
            if "input_value" not in build_config:
                build_config["input_value"] = {"value": "", "input_types": ["Message"]}
            if "agent_description" not in build_config:
                build_config["agent_description"] = {"value": "A helpful assistant with access to the following tools:"}
            if "work_plan_prompt" not in build_config:
                build_config["work_plan_prompt"] = {"value": None, "input_types": ["Message"]}
            if "planning_prompt" not in build_config:
                build_config["planning_prompt"] = {"value": None, "input_types": ["Message"]}

            if "critic_prompt" not in build_config:
                build_config["critic_prompt"] = {"value": None, "input_types": ["Message"]}
            
            missing_keys = [key for key in default_keys if key not in build_config]
            if missing_keys:
                msg = f"Missing required keys in build_config: {missing_keys}"
                raise ValueError(msg)
        if (
            isinstance(self.agent_llm, str)
            and self.agent_llm in MODEL_PROVIDERS_DICT
            and field_name in MODEL_DYNAMIC_UPDATE_FIELDS
        ):
            provider_info = MODEL_PROVIDERS_DICT.get(self.agent_llm)
            if provider_info:
                component_class = provider_info.get("component_class")
                component_class = self.set_component_params(component_class)
                prefix = provider_info.get("prefix")
                if component_class and hasattr(component_class, "update_build_config"):
                    # Call each component class's update_build_config method
                    # remove the prefix from the field_name
                    if isinstance(field_name, str) and isinstance(prefix, str):
                        field_name = field_name.replace(prefix, "")
                    build_config = await update_component_build_config(
                        component_class, build_config, field_value, "model_name"
                    )
        return dotdict({k: v.to_dict() if hasattr(v, "to_dict") else v for k, v in build_config.items()})
